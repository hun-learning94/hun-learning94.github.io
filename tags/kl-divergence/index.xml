<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>KL divergence on Hun Learning</title>
    <link>/tags/kl-divergence/</link>
    <description>Recent content in KL divergence on Hun Learning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Aug 2020 10:00:00 +0900</lastBuildDate>
    
	<atom:link href="/tags/kl-divergence/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Variational Inference and Bayesian Gaussian Mixture Model</title>
      <link>/posts/2020-08-25-variational-inference/</link>
      <pubDate>Tue, 25 Aug 2020 10:00:00 +0900</pubDate>
      
      <guid>/posts/2020-08-25-variational-inference/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Forward and Reverse KL divergence</title>
      <link>/posts/2020-08-24-forward-and-reverse-kl-divergence/</link>
      <pubDate>Mon, 24 Aug 2020 08:00:00 +0900</pubDate>
      
      <guid>/posts/2020-08-24-forward-and-reverse-kl-divergence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpretation of MLE in terms of KL divergence</title>
      <link>/posts/bayesian-ml/week5/05-mle-minimizes-kl-divergence/</link>
      <pubDate>Tue, 11 Aug 2020 11:00:00 +0900</pubDate>
      
      <guid>/posts/bayesian-ml/week5/05-mle-minimizes-kl-divergence/</guid>
      <description>Suppose that the true density of a random variable $x$ is $p(x)$. Since this is unknown, we can try to come up with an approximation $q(x)$. Then KL divergences is a good measure of mismatch between $p$ and $q$ distribution.
$$ \begin{align*} \text{KL divergence:}\quad KL(p||q) = \int p(x)\log \dfrac{p(x)}{q(x)}dx \end{align*} $$ From the formula we can see that KL divergence is a weighted average, with wighted $p(x)$, of an error induced by approximation ($\log p(x) - \log q(x)$).</description>
    </item>
    
    <item>
      <title>Note on Kullback-Leibler Divergence</title>
      <link>/posts/bayesian-ml/week5/04-note-on-kullback-leibler-divergence/</link>
      <pubDate>Tue, 11 Aug 2020 11:00:00 +0900</pubDate>
      
      <guid>/posts/bayesian-ml/week5/04-note-on-kullback-leibler-divergence/</guid>
      <description>How do we quantify an amount of information that some data $x$ contains? If the data is pretty much expected than it tells nothing new to us. But if it is so rare then it has some value. In this sense, we can think of an amount of information as a &amp;ldquo;degree of surprise&amp;rdquo;, and define
$$ \text{information content of data $x$:}\quad h(x) = -\log p(x) $$ where the logarithm ensures $h(x,y)=h(x)+h(y) \Leftrightarrow p(x,y)=p(x)p(y)$, and the negative sign makes $h(x)\geq 0$.</description>
    </item>
    
  </channel>
</rss>