<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Bayesian Hierarchical Modeling and its Applications - Hun Learning</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="Example article description">
		<meta property="og:title" content="Bayesian Hierarchical Modeling and its Applications" />
<meta property="og:description" content="Example article description" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/bayesian-ml/week3/03-bayesian-hierarchical-modeling-and-applications/" />
<meta property="article:published_time" content="2020-08-03T08:00:00+09:00" />
<meta property="article:modified_time" content="2020-08-03T08:00:00+09:00" />

		<meta itemprop="name" content="Bayesian Hierarchical Modeling and its Applications">
<meta itemprop="description" content="Example article description">
<meta itemprop="datePublished" content="2020-08-03T08:00:00+09:00" />
<meta itemprop="dateModified" content="2020-08-03T08:00:00+09:00" />
<meta itemprop="wordCount" content="1423">



<meta itemprop="keywords" content="Bayesian Hierarchy," />

	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed">
		<a class="logo__link" href="/" title="Hun Learning" rel="home">
			<div class="logo__item logo__imagebox">
					<img class="logo__img" src="/mypic2.jpg">
				</div><div class="logo__item logo__text">
					<div class="logo__title">Hun Learning</div>
					<div class="logo__tagline">In Search Of The Truth Projected Onto A Finite Dimension</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/posts/about/">
				
				<span class="menu__text">Author</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
	<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
</script>
<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "all" } }
  });
</script>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Bayesian Hierarchical Modeling and its Applications</h1>
			<p class="post__lead">여러 모수를 한 묶음으로 묶어주는 Hyperparameter</p>
			<div class="post__meta meta"><div class="meta__item-author meta__item">
	<svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2 0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class="meta__text">Kang Gyeonghun</span>
</div>
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2020-08-03T08:00:00&#43;09:00">2020-08-03</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/probabilistic-machine-learning/" rel="category">Probabilistic Machine Learning</a>
	</span>
</div></div>
		</header>
		<figure class="post__thumbnail">
			<img src="/cover/hierarchy.jpg" alt="Bayesian Hierarchical Modeling and its Applications">
		</figure><div class="content post__content clearfix">
			<h2 id="review-full-conditional-posterior-for-normal-likelihood">Review: Full conditional posterior for normal likelihood</h2>
<p>일단 정규분포의 semi-conjugate prior에 대한 내용을 다시 정리해보자.</p>
<ul>
<li>$p(\theta\mid\sigma^2, \mathbf{D}) = dnorm(\theta, \mu_n, \tau_n^2)$</li>
<li>$\mu_n= \dfrac{1/\tau_0^2}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}\mu_0 + \dfrac{\frac{n}{\sigma^2}}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}\bar{x}$</li>
<li>$\tau_n^2 = \dfrac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}$</li>
<li>$p(\sigma^2\mid\theta, \mathbf{D}) = dinv\Gamma(\sigma^2, v_n, \dfrac{1}{v_n}(v_0\sigma_0^2+\sum (y_i-\theta)^2)$</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="two-group-comparison-math-scores">Two Group Comparison: Math scores</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(ggplot2)
<span style="color:#a6e22e">library</span>(cowplot)

school1 <span style="color:#f92672">=</span> <span style="color:#a6e22e">dget</span>(<span style="color:#e6db74">&#39;http://www2.stat.duke.edu/~pdh10/FCBS/Inline/y.school1&#39;</span>)
school2 <span style="color:#f92672">=</span> <span style="color:#a6e22e">dget</span>(<span style="color:#e6db74">&#39;http://www2.stat.duke.edu/~pdh10/FCBS/Inline/y.school2&#39;</span>)

df <span style="color:#f92672">=</span> <span style="color:#a6e22e">data.frame</span>(school <span style="color:#f92672">=</span> <span style="color:#a6e22e">c</span>(<span style="color:#a6e22e">rep</span>(<span style="color:#e6db74">&#39;s1&#39;</span>, <span style="color:#a6e22e">length</span>(school1)),<span style="color:#a6e22e">rep</span>(<span style="color:#e6db74">&#39;s2&#39;</span>, <span style="color:#a6e22e">length</span>(school2))),
               score <span style="color:#f92672">=</span> <span style="color:#a6e22e">c</span>(school1, school2)
               )

<span style="color:#a6e22e">ggplot</span>(df, <span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>school, y<span style="color:#f92672">=</span>score))<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_boxplot</span>(<span style="color:#a6e22e">aes</span>(fill<span style="color:#f92672">=</span>school))<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#39;Math scores comparison&#39;</span>)<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">theme_cowplot</span>()
</code></pre></div><p><img src="/image/Rplot001.png" alt="Rplot001"></p>
<p>통계학이 필요한 이유는 이런 &ldquo;애매한&rdquo; 차이 때문이다. 딱 봐도 눈에 훤히 보이게 다르면 누가 통계학자를 찾겠나.</p>
<p>빈도론자들이 하는 two-sample t-test는 다들 배웠을 것이다. 그런데 어떻게 두 집단의 평균이 완전히 똑같거나 아니면 절대 다르거나 둘 중 하나만 가능하냐, 어색하다, 생각할 수도 있다. 그러니까 수식으로 말하면 빈도론자들의 방법은, 학교1의 모평균을 $\hat{\theta_1} = w \bar{y_1}+(1-w) \bar{y_2}$로 추정하고, $w$는 다음과 같이 정한다는 것이다.</p>
<!-- raw HTML omitted -->
<p>$$
w = \begin{cases}
1 \quad &amp;if \quad p-value&lt;0.05\<br>
\dfrac{n_1}{n_1+n_2} \quad &amp;o.w.
\end{cases}
$$
<!-- raw HTML omitted --></p>
<p>이렇게 너무 극단적으로 정하기보다, 학교들의 상대적인 표본 수, 학교들 표본의 sampling variability 등 다양한 요소들을 고려해서 $w$가 취할 수 있는 값들의 연속적인 분포를 한번 그려보자는 것이 베이지안 추론의 접근이다.</p>
<p>베이지안 프레임워크에 맞춰서 이 문제를 해석하면 다음과 같다.</p>
<p><strong><!-- raw HTML omitted -->Likelihood</strong></p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
y_{1,i} &amp;\sim N(\mu+\delta, \sigma^2)\\\<br>
y_{2,i} &amp;\sim N(\mu-\delta, \sigma^2)\\\<br>
p(D|\mu,\delta, \sigma^2)&amp;= \prod_{i=1}^{n_1}dnorm(y_{1,i}, \mu+\delta, \sigma^2)
\prod_{j=1}^{n_2}dnorm(y_{2,j}, \mu-\delta, \sigma^2)
\end{align}
$$</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted --><strong>Prior</strong></p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
p(\mu, \delta, \sigma^2) &amp;= p(\mu)p(\delta)p(\sigma^2)\\\<br>
\mu &amp;\sim N(\mu_0, \gamma_0^2)\\\<br>
\delta &amp;\sim N(\delta_0, \tau_0^2)\\\<br>
\sigma^2 &amp;\sim inv\Gamma(\nu_0/2, \nu_0\sigma^2_0/2)
\end{align}
$$</p>
<!-- raw HTML omitted -->
<p><strong><!-- raw HTML omitted -->Full conditional posterior</strong></p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
\mu \mid D, \delta, \sigma^2 &amp;\sim \mathcal{N}(\mu_n, \gamma_n^2)\\\<br>
\mu_n &amp;= \gamma_n^2 \times \left[ \mu_0 / \gamma_0^2 + \sum_{i = 1}^{n_1} (y_{i, 1} - \delta) / \sigma^2 + \sum_{i = 1}^{n_2}(y_{i, 2} + \delta) / \sigma^2 \right]\\\<br>
\gamma_n^2 &amp;= \left[ 1/\gamma_0^2 + (n_1 + n_2) / \sigma^2  \right]^{-1}\\\<br>
\delta \mid D, \mu, \sigma^2 &amp;\sim \mathcal{N}(\delta_n, \tau_n^2)\\\<br>
\delta_n &amp;= \tau_n^2 \times \left[ \delta_0 / \tau_0^2 + \sum_{i = 1}^{n_1} (y_{i, 1} - \mu) / \sigma^2 - \sum_{i = 1}^{n_2} (y_{i, 2} - \mu) / \sigma^2 \right]\\\<br>
\tau_n^2 &amp;= \left[ 1 / \tau_0^2 + (n_1 + n_2) / \sigma^2 \right]^{-1}\\\<br>
\sigma^2 \mid D, \mu, \delta &amp;\sim inv\Gamma(\nu_n / 2, \sigma_n^2 \nu_n / 2)\\\<br>
\nu_n &amp;= \nu_0 + n_1 + n_2\\\<br>
\nu_n \sigma_n^2 &amp;= \nu_0 \sigma_0^2 + \sum_{i = 1}^{n_1} (y_{i, 1} - \left[\mu + \delta \right])^2 + \sum_{i = 1}^{n_2} (y_{i, 2} - \left[ \mu - \delta \right])^2
\end{align}
$$</p>
<!-- raw HTML omitted -->
<p>수식만 보면 기겁을하게 생겼는데 사실 다 노말 분포의 full conditional posterior가 뭔지만 알면 큰 어려움 없이 유도할 수 있다. 이제 이렇게 각 모수에 대한 full conditional posterior를 구했으니 깁스 샘플러를 돌릴 수 있다. 코드는 교재를 참고하자.</p>
<p>중요한 것은, 베이지안에서 두 그룹의 평균 차이가 유의미한지 보는 것은 결국 $\delta$의 사후분포를 보는 것과 같다는 점이다. 깁스 샘플러의 매번 반복 횟수마다 모수를 생성하고, 그 모수에 대해 샘플을 생성해, 다음과 같은 통계량 $\hat{p}(d&gt;0|D), \hat{p}(Y_1 &gt; Y_2|D)$을 전체 샘플 중에서의 상대빈도로 구할 수 있다.</p>
<p><img src="/image/pic01.PNG" alt="pic01"></p>
<p>(Hoff, p.130)</p>
<h2 id="brbayesian-hierarchical-model-for-multiple-comparisons"><!-- raw HTML omitted -->Bayesian Hierarchical Model for Multiple Comparisons</h2>
<p>만일 두 개 이상의 여러 집단 ${\mathbf{Y}_1, \mathbf{Y}_2, &hellip;, \mathbf{Y}_m}$간의 평균 ${\theta_1, \theta_2, &hellip;, \theta_m}$을 비교한다고 해보자. 이때 데이터에 대해 다음과 같은 가정을 세운다고 하자.</p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
p(y_j\mid \theta_j, \sigma^2) &amp;= dnorm(y, \theta_j, \sigma^2) \quad \text{within group model}\\\<br>
p(\theta_j\mid \mu, \tau^2) &amp;= dnorm(\theta_j, \mu, \tau^2) \quad \text{between group model}
\end{align}
$$</p>
<!-- raw HTML omitted -->
<p>데이터에 대한 이러한 모델은 다음의 가정을 내포하고 있다.</p>
<ol>
<li>한 그룹 내에서 관측치들은 동일한 분포를 따르는 독립인 샘플이다.</li>
<li>각 그룹별 평균도 또한 동일한 분포를 따르는 독립인 샘플이다.</li>
</ol>
<p>이러한 가정이 타당한 이유는 <strong>de Finetti&rsquo;s theorem</strong>에서 찾을 수 있다. 어떤 데이터 열의 확률 $p({x_1, x_2, &hellip;, x_n})$ 이 그 데이터 열의 순서를 바꿔도, 즉 모든 가능한 permutation에 대해 그 데이터 열의 확률이 변하지 않을 때 이 데이터들이 <strong>exchangeable</strong>하다고 한다. 드핀티 정리란, 이렇게 exchangeable한 데이터들은 서로 독립이 아닐지라도 어떤 모수가 주어지면 조건부 독립으로 볼 수 있다는 말이다.</p>
<p><!-- raw HTML omitted --><strong>de Finetti&rsquo;s theorem</strong></p>
<p>다음 두 조건은 동치이다.</p>
<ul>
<li>${x_1, x_2, &hellip;, x_n}$은 exchangeable하다.</li>
<li>어떤 모수 $\boldsymbol{\theta} \sim p(\boldsymbol{\theta})$가 있어 다음이 성립한다; ${x_1, x_2, &hellip;, x_n \mid \boldsymbol{\theta} } \sim^{iid} p(\mathbf{x}\mid \boldsymbol{\theta})$</li>
</ul>
<!-- raw HTML omitted -->
<p>여러 집단의 평균을 비교하는 문제에서도, 한 집단 내에 샘플들은 서로 exchangeable하고, 또 샘플의 모평균끼리도 exchangeable하다. 때문에 (24), (25)와 같은 가정이 충분히 가능한 것이다.</p>
<p>위 가정에서 모수 $\boldsymbol{\theta}$는 $\mu, \tau^2, \sigma^2$이다. 이 모수에 대해 다음의 prior를 줄 수 있다.</p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
\sigma^2 &amp;\sim inv\Gamma(\nu_0 / 2, \sigma_0^2 \nu_0 / 2) \\\<br>
\tau^2 &amp;\sim inv\Gamma(\eta_0 / 2, \tau_0^2 \eta_0 / 2) \\\<br>
\mu &amp;\sim N(\mu_0, \gamma_0^2)
\end{align}
$$</p>
<!-- raw HTML omitted -->
<p>뭔가 식이 되게 많아져서 복잡해보이는데, 이걸 그림으로 나타내면 다음과 같다.</p>
<!-- raw HTML omitted -->
<p>이렇게 Likelihood와 Prior를 세우고 나면 Posterior는 어떻게 구할 수 있을까? 일단 써보자.</p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
p(\theta_{j=1}^m, \mu, \tau^2, \sigma^2 \mid \mathbf{D}) 
&amp;\propto p( \mu, \tau^2, \sigma^2); p(\theta_{j=1}^m\mid \mu, \tau^2, \sigma^2); p(\mathbf{D}\mid\theta_{j=1}^m, \mu, \tau^2, \sigma^2)\\\<br>
&amp;= p(\mu)p(\tau^2)p(\sigma^2);\prod_{j=1}^m p(\theta_j \mid \mu, \tau^2)  \prod_{j=1}^m \prod_{i=1}^{n_j} p(y_{j,i}\mid \theta_j, \sigma^2)
\end{align}
$$</p>
<!-- raw HTML omitted -->
<p>(29)에서 (30)으로 넘어가는 과정에서 갸우뚱했을 것이다. 하나씩 살펴보자.</p>
<ol>
<li>$p( \mu, \tau^2, \sigma^2) = p(\mu)p(\tau^2)p(\sigma^2)$ 
이건 prior를 독립으로 줘서 그렇다. 자명함.</li>
<li>$p(\theta_{j=1}^m\mid \mu, \tau^2, \sigma^2)=\prod_{j=1}^m p(\theta_j \mid \mu, \tau^2)$
$\theta_j$는 그룹의 평균이다. 그룹의 평균은 우리가 (25)에서 오직 $\mu, \tau^2$에만 의존한다고 했다. 때문에 $\sigma^2$와 독립이므로, 조건 항에서 뺄 수 있는 것이다. 이는 위에 그림을 보면 잘 보인다. 위의 그림에서 화살표로 직접적으로 이어져있지 않는 변수들은 모두 독립이다. 직관적으로 생각해보면, $\mu, \tau^2$의 값을 알면 우리는 $\theta_j$의 분포를 아는 것이니 $\theta_j$에 대해서도 어느정도 알게 된다. 그러나 $\sigma^2$를 안다고 해서 아무것도 변하는 것은 없다.</li>
<li>$p(\mathbf{D}\mid\theta_{j=1}^m, \mu, \tau^2, \sigma^2) = \prod_{j=1}^m \prod_{i=1}^{n_j} p(y_{j,i}\mid \theta_j, \sigma^2)$
$\mu, \tau^2$는 화살표를 두 번 타고 가면 $Y$에 이어지기 때문에 잘 납득이 안 될 수도 있다. 그러나 $Y_j$에 대하여 $\mu, \tau^2$가 줄 수 있는 정보는 이미 $\theta_j$에 모두 담겨 있다. 때문에 $\theta_j$를 알고 있다면 $\mu, \tau^2$ 따위 알던 모르던 상관이 없는 것이다.</li>
</ol>
<p>이제 식 (30)을 이용하면 각 모수의 full conditional posterior가 다음에 비례함을 쉽게 알 수 있다. 예컨대 $\mu$의 full conditional posterior이라는 것은 데이터와 다른 모수들이 모두 상수로 주어졌다는 것이다. 때문에 식 (30)에서 $\mu$가 포함되지 않은 식은 모두 상수 취급하여 $\propto$로 없애버릴 수 있는 것이다.</p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
p(\mu \mid \theta_{j=1}^m, \tau^2, \sigma^2, \mathbf{D}) 
&amp;\propto p(\mu)\prod_{j=1}^m p(\theta_j \mid \mu, \tau^2)\\\<br>
p(\tau^2 \mid \theta_{j=1}^m, \mu, \sigma^2, \mathbf{D}) 
&amp;\propto p(\tau^2)\prod_{j=1}^m p(\theta_j \mid \mu, \tau^2)\\\<br>
p(\theta_j \mid \mu, \tau^2, \sigma^2, \mathbf{D}) 
&amp;\propto p(\theta_j\mid\mu, \tau^2) \prod_{j=1}^m \prod_{i=1}^{n_j} p(y_{j,i}\mid \theta_j, \sigma^2)\\\<br>
p(\sigma^2 \mid \theta_{j=1}^m, \mu, \tau^2, \mathbf{D}) 
&amp;\propto p(\sigma^2) \prod_{j=1}^m \prod_{i=1}^{n_j} p(y_{j,i}\mid \theta_j, \sigma^2)
\end{align}
$$</p>
<!-- raw HTML omitted -->
<p>이 경우 각 모수들은 어차피 노말 혹은 인버스 감마를 따르기 때문에, 사후분포도 다음과 같이 유도할 수 있다.</p>
<p>$$
\begin{align}
{\mu \mid \theta_{j=1}^m, \tau^2} &amp;\sim N\left(\frac{m\bar{\theta} / \tau^2 + \mu_0 / \gamma_0^2}{m/\tau^2 + 1 / \gamma_0^2}, \left[m / \tau^2 + 1 / \gamma_0^2 \right]^{-1}\right) \\\<br>
{\tau^2 \mid \theta_{j=1}^m, \mu} &amp;\sim inv\Gamma\left(\frac{\eta_0 + m}{2}, \frac{\eta_0\tau_0^2 + \sum_{j = 1}^{m}(\theta_j - \mu)^2}{2} \right) \\\<br>
{\theta_j \mid \mathbf{D}, \sigma^2} &amp;\sim N\left(\frac{n_j\bar{y}_j / \sigma^2 + 1 / \tau^2}{n_j / \sigma^2 + 1 / \tau^2}, \left[ n_j / \sigma^2 + 1 / \tau^2 \right]^{-1} \right) \\\<br>
{\sigma^2 \mid \theta_{j=1}^m, \mathbf{D}} &amp;\sim inv\Gamma\left(\frac{1}{2}\left[ \nu_0 + \sum_{j = 1}^m n_j\right], \frac{1}{2}\left[\nu_0\sigma_0^2 + \sum_{j = 1}^{m} \left( \sum_{i = 1}^{n_j} (y_{i, j} - \theta)^2 \right) \right] \right)
\end{align}
$$</p>
<!-- raw HTML omitted -->
<p>이제 이 식들을 이용해 Gibbs Sampler를 진행하면 된다. 이때 우리가 세운 모델에서는 각 집단의 평균들이 모두 동일한 분포에서 가정하였기 때문에, 표본평균에 비하여 생성된 $\theta_j$의 평균 $E[\theta_j|\mathbf{D}]$은 서로 가까워질 것이다. 즉 표본평균이 높은 그룹은 낮게, 낮은 그룹은 높게 &ldquo;보정&quot;이 될 것이고, 특히 표본 수가 낮아서 sampling variability가 클수록 이런 보정의 정도가 클 것이다.</p>
<p>무슨 말인지 예를 들어보자. 다음과 같은 데이터가 있다고 하자.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(dplyr)
Y.school.mathscore <span style="color:#f92672">=</span> <span style="color:#a6e22e">dget</span>(<span style="color:#e6db74">&#39;http://www2.stat.duke.edu/~pdh10/FCBS/Inline/Y.school.mathscore&#39;</span>)

school.df <span style="color:#f92672">=</span> Y.school.mathscore <span style="color:#f92672">%&gt;%</span> as.data.frame <span style="color:#f92672">%&gt;%</span> as_tibble

meanscore <span style="color:#f92672">=</span> school.df <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">group_by</span>(school) <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">summarise</span>(meanscore <span style="color:#f92672">=</span> <span style="color:#a6e22e">mean</span>(mathscore), count<span style="color:#f92672">=</span><span style="color:#a6e22e">n</span>()) <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">mutate</span>(rank<span style="color:#f92672">=</span><span style="color:#a6e22e">rank</span>(meanscore))

school.toplot <span style="color:#f92672">=</span> school.df <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">left_join</span>(meanscore, by<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;school&#39;</span>) <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">mutate</span>(minscore <span style="color:#f92672">=</span> <span style="color:#a6e22e">min</span>(mathscore), maxscore<span style="color:#f92672">=</span><span style="color:#a6e22e">max</span>(mathscore))

p1 <span style="color:#f92672">=</span> <span style="color:#a6e22e">ggplot</span>(school.toplot, <span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>rank, y<span style="color:#f92672">=</span>mathscore, group<span style="color:#f92672">=</span>school))<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">stat_summary</span>(fun.min <span style="color:#f92672">=</span>min, fun.max <span style="color:#f92672">=</span> max, fun<span style="color:#f92672">=</span>mean)<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">theme_cowplot</span>()

p2 <span style="color:#f92672">=</span> <span style="color:#a6e22e">ggplot</span>(meanscore, <span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>count, y<span style="color:#f92672">=</span>meanscore))<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_point</span>(shape<span style="color:#f92672">=</span><span style="color:#ae81ff">21</span>, fill <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;lightgray&#34;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;black&#34;</span>, size <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>)<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">theme_cowplot</span>()

<span style="color:#a6e22e">plot_grid</span>(p1, p2)
</code></pre></div><p><img src="/image/Rplot002.png" alt="Rplot002"></p>
<p>교재에 나온 코드 대로 깁스 샘플러를 돌려보면 다음과 같이 <strong>shrinkage</strong>가 발생하며, 특히 표본 수가 작을수록 보정이 더 많이 됨을 알 수 있다.</p>
<p><img src="/image/pic03.PNG" alt="pic03"></p>
<p>(Hoff, p.141)</p>
<h3 id="brhw-필수-components-of-variance-in-hierarchical-model"><!-- raw HTML omitted -->(HW, 필수) Components of variance in hierarchical model</h3>
<p>교재 (Hoff, 2009) p.241의 연습문제 8.1 한번 생각해보기. 꼭 제출할 필요는 없음.</p>
<h3 id="brhw-필수-the-eight-schools-comparison"><!-- raw HTML omitted -->(HW, 필수) The eight schools comparison</h3>
<p>다음의 사이트를 참조하여</p>
<ol>
<li>베이지안 모델을 이해해서 그림으로 그려보고,</li>
<li>이를 시뮬레이션하는 stan 파일을 만들고</li>
<li>rstan으로 직접 돌려보고 그 결과를 해석하자.</li>
</ol>
<p><a href="https://www.datascienceblog.net/post/machine-learning/probabilistic_programming/">https://www.datascienceblog.net/post/machine-learning/probabilistic_programming/</a></p>
<p>(Stan 튜토리얼 <a href="https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/applied-bayesian-statistics/#validation-and-inference">https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/applied-bayesian-statistics/#validation-and-inference</a>)
(Stan이 별로면 그냥 JAGS나 다른거 써도 되고 직접 코드 짜도 된다.)</p>
<h3 id="brhw-권장-predicting-batting-averages-어려움"><!-- raw HTML omitted -->(HW, 권장) Predicting batting averages (어려움)</h3>
<p>다음의 사이트를 참조하여</p>
<ol>
<li>베이지안 모델을 이해해서 그림으로 그려보고,</li>
<li>이를 시뮬레이션하는 stan 파일을 만들고</li>
<li>rstan으로 직접 돌려보고 그 결과를 해석하자.</li>
</ol>
<p><a href="http://www.probabilaball.com/2016/05/lets-code-mcmc-for-hierarchical-model.html">http://www.probabilaball.com/2016/05/lets-code-mcmc-for-hierarchical-model.html</a></p>
<h3 id="brhw-권장-hierarchical-model-for-economic-growth-더-어려움"><!-- raw HTML omitted -->(HW, 권장) Hierarchical model for economic growth (더 어려움)</h3>
<p>다음의 사이트와 첨부한 논문을 참조하여</p>
<ol>
<li>베이지안 모델을 이해해서 그림으로 그려보고,</li>
<li>이를 시뮬레이션하는 stan 파일을 만들고</li>
<li>rstan으로 직접 돌려보고 그 결과를 해석하자.</li>
</ol>
<p><a href="https://jrnold.github.io/bugs-examples-in-stan/corporatism.html">https://jrnold.github.io/bugs-examples-in-stan/corporatism.html</a></p>
<p>참고로 이 데이터를 받으려면 다음과 같이 bayesjackman 패키지를 설치해야 한다.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">install.packages</span>(<span style="color:#e6db74">&#34;devtools&#34;</span>)
devtools<span style="color:#f92672">::</span><span style="color:#a6e22e">install_github</span>(<span style="color:#e6db74">&#34;jrnold/jackman-bayes&#34;</span>, subdir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;bayesjackman&#34;</span>)
<span style="color:#a6e22e">data</span>(<span style="color:#e6db74">&#34;corporatism&#34;</span>, package <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;bayesjackman&#34;</span>)
</code></pre></div><p>이렇게하면 corporatism 데이터가 작업환경에 변수로 저장이 되어있다.</p>
<h2 id="brreferences"><!-- raw HTML omitted -->References</h2>
<ol>
<li>Hoff, P. D. (2009). A first course in Bayesian statistical methods. New York: Springer.</li>
<li>그 외 위에 있는 링크들</li>
</ol>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/bayesian-hierarchy/" rel="tag">Bayesian Hierarchy</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="Kang Gyeonghun avatar" src="/mypic1.jpg" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About Kang Gyeonghun</span>
	</div>
	<div class="authorbox__description">
		I study statistics, machine learning, data science or whatever that concerns making inference on infinitie dimension from a limited sample in fintie dimension. This blog is an archive of my journey of study.
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bayesian-ml/week3/02-mcmc-approximation-for-bayesian-posterior/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">(MCMC) 베이지안 사후분포 근사를 위한 MCMC 방법론</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bayesian-ml/week5/01-k-means-clustering/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">K-means clustering</p>
		</a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hun-learning94" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/posts/how-to-add-rtools-to-windows-path-env/">Rtools를 윈도우 환경변수 PATH에 추가하는 방법</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-25-variational-inference/">Variational Inference and Bayesian Gaussian Mixture Model</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-24-forward-and-reverse-kl-divergence/">Forward and Reverse KL divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/bayesian-ml/week5/05-mle-minimizes-kl-divergence/">Interpretation of MLE in terms of KL divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/bayesian-ml/week5/04-note-on-kullback-leibler-divergence/">Note on Kullback-Leibler Divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-11-bayesian-networks-directed-acyclical-graphs/">Bayesian Networks (Directed Acyclical Graphs)</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/adaboost/" title="ADABOOST">ADABOOST</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/adaptive-basis-model/" title="Adaptive Basis Model">Adaptive Basis Model</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayes-rule/" title="Bayes Rule">Bayes Rule</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-gmm/" title="Bayesian GMM">Bayesian GMM</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-hierarchy/" title="Bayesian Hierarchy">Bayesian Hierarchy</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-networks/" title="Bayesian Networks">Bayesian Networks</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bootstrap/" title="Bootstrap">Bootstrap</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/cart/" title="CART">CART</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/clustering/" title="Clustering">Clustering</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/conjugacy/" title="Conjugacy">Conjugacy</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/d-seperation/" title="D-seperation">D-seperation</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/determinant/" title="Determinant">Determinant</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/diagonalization/" title="Diagonalization">Diagonalization</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/em-algorithm/" title="EM algorithm">EM algorithm</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ensemble-learning/" title="Ensemble Learning">Ensemble Learning</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/frequentist/" title="Frequentist">Frequentist</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gaussian-mixtures/" title="Gaussian Mixtures">Gaussian Mixtures</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/generalized-additive-models/" title="Generalized Additive Models">Generalized Additive Models</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gibbs-sampling/" title="Gibbs Sampling">Gibbs Sampling</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/intro-to-statistical-learning/" title="Intro to Statistical Learning">Intro to Statistical Learning</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/jacobian/" title="Jacobian">Jacobian</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/k-cv/" title="k-CV">k-CV</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/k-means/" title="K-means">K-means</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/kernel/" title="Kernel">Kernel</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/kl-divergence/" title="KL divergence">KL divergence</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lagrangian-duality/" title="Lagrangian Duality">Lagrangian Duality</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lasso/" title="Lasso">Lasso</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/latent-variable/" title="Latent Variable">Latent Variable</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lda/" title="LDA">LDA</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/linear-adjoint/" title="Linear Adjoint">Linear Adjoint</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/logistic-regression/" title="Logistic Regression">Logistic Regression</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/markov-chain/" title="Markov Chain">Markov Chain</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/matrix-derivatives/" title="Matrix Derivatives">Matrix Derivatives</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mcmc/" title="MCMC">MCMC</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/metropolis-hastings/" title="Metropolis Hastings">Metropolis Hastings</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mse/" title="MSE">MSE</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/multivariate-normal/" title="Multivariate Normal">Multivariate Normal</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/naive-bayes-classifier/" title="Naive Bayes Classifier">Naive Bayes Classifier</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ols/" title="OLS">OLS</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/pca/" title="PCA">PCA</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/posterior-approximation/" title="Posterior Approximation">Posterior Approximation</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/rcpp/" title="Rcpp">Rcpp</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/regression-splines/" title="Regression Splines">Regression Splines</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ridge/" title="Ridge">Ridge</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/self-adjoint/" title="Self Adjoint">Self Adjoint</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/similar-matrices/" title="Similar Matrices">Similar Matrices</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/singular-value-decomposition/" title="Singular Value Decomposition">Singular Value Decomposition</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/stochastic-process/" title="Stochastic Process">Stochastic Process</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/svm/" title="SVM">SVM</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/vector-derivatives/" title="vector derivatives">vector derivatives</a>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2020 Kang Gyeonghun.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>