<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>K-means clustering - Hun Learning</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="Example article description">
		<meta property="og:title" content="K-means clustering" />
<meta property="og:description" content="Example article description" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/bayesian-ml/week5/01-k-means-clustering/" />
<meta property="article:published_time" content="2020-08-10T06:00:00+09:00" />
<meta property="article:modified_time" content="2020-08-10T06:00:00+09:00" />

		<meta itemprop="name" content="K-means clustering">
<meta itemprop="description" content="Example article description">
<meta itemprop="datePublished" content="2020-08-10T06:00:00+09:00" />
<meta itemprop="dateModified" content="2020-08-10T06:00:00+09:00" />
<meta itemprop="wordCount" content="662">



<meta itemprop="keywords" content="K-means,Clustering," />

	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed">
		<a class="logo__link" href="/" title="Hun Learning" rel="home">
			<div class="logo__item logo__imagebox">
					<img class="logo__img" src="/mypic2.jpg">
				</div><div class="logo__item logo__text">
					<div class="logo__title">Hun Learning</div>
					<div class="logo__tagline">In Search Of The Truth Projected Onto A Finite Dimension</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/posts/about/">
				
				<span class="menu__text">Author</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
	<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
</script>
<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "all" } }
  });
</script>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">K-means clustering</h1>
			<p class="post__lead">A simple clustering algorithm from a simple approach</p>
			<div class="post__meta meta"><div class="meta__item-author meta__item">
	<svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2 0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class="meta__text">Kang Gyeonghun</span>
</div>
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2020-08-10T06:00:00&#43;09:00">2020-08-10</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/probabilistic-machine-learning/" rel="category">Probabilistic Machine Learning</a>
	</span>
</div></div>
		</header>
		<figure class="post__thumbnail">
			<img src="/cover/clustering.jpg" alt="K-means clustering">
		</figure><div class="content post__content clearfix">
			<p>Gaussian mixture model is a widely used probabilistic model. For inference (model learning), we may use either EM algorithm which is a MLE approach or use Bayesian approach, which leads to variational inference. We would study this topic next week. For now, let us introduce one of the well-known nonparameteric methods for unsupervised learning, and introduce Gaussian mixture as a parametric counterpart.</p>
<h2 id="brk-means-clustering"><!-- raw HTML omitted -->K-means clustering</h2>
<p>Let us suppose that we know the total number of clusters is fixed as $K$. The objective function of the cluster problem is</p>
<!-- raw HTML omitted -->
<p>$$
J = \sum_{n=1}^K\sum_{k=1}^K r_{nk} |\mathbf{x_n}-\boldsymbol{\mu_k}|^2
$$
<!-- raw HTML omitted --></p>
<p>where $\boldsymbol{\mu_k}$ is a centroid of $k$th cluster, $\mathbf{x_n}$ is a data vector, and  $r_{nk}=1$ if $\mathbf{x_m}$ is in $k$th cluster, and $r_{nj}=0; ^\forall j\neq k$. Our goal here is to choose not only the cluster center (centroids) $\boldsymbol{\mu_k}$, but also the cluster assignment $r_{nk}$ so as to minimize $J$. K-means algorithm achieves this with an iterative algorithm where the centroids and the assignments update successively.</p>
<p>Note that the objective function is given as a sum of Euclidean distances of each vector from its corresponding centroids. For a fixed set of centroids $\boldsymbol{\mu_k}$, each vector should be assigned to the nearest cluster center to minimize $J$. In turn, for a given cluster assignment, we have to choose $\boldsymbol{\mu_k}$ so that $|\mathbf{x_n}-\boldsymbol{\mu_k}|^2$ is minimized. Taking a derivative with respect to $\boldsymbol{\mu_k}$, we see that $2\sum_{n=1}^Nr_{nk}(\mathbf{x_n}-\boldsymbol{\mu_k})=0$ if the centroids is indeed a mean of all the data vectors in $k$th cluster.</p>
<p>All in all, K-means algorithm proceeds as follows;</p>
<ol>
<li>
<p><strong>Update assignments:</strong> Given the centroids $\boldsymbol{\mu_k}$, update $r_{nk}$ for each data vector as</p>
<!-- raw HTML omitted -->
<p>$$
r_{nk}=
\begin{cases}
1 &amp;\quad k=\arg\min_j |\mathbf{x_n}-\boldsymbol{\mu_k}|^2\\\<br>
0 &amp;\quad o.w.
\end{cases}
$$
<!-- raw HTML omitted --></p>
</li>
<li>
<p><strong>Update centroids:</strong> Given $r_{nk}$, update the centroids $\boldsymbol{\mu_k}$ as</p>
<!-- raw HTML omitted -->
<p>$$
\boldsymbol{\mu_k}=\dfrac{\sum_{n}r_{nk}\mathbf{x_n}}{\sum_n r_{nk}}
$$
<!-- raw HTML omitted --></p>
<p>which is simply just a mean of each cluster.</p>
</li>
</ol>
<p>Some points worth mentioned;</p>
<ol>
<li>Computational cost of K-means clustering is roughly $O(NK)$, since at each iteration we have to compute Euclidean distances of $N$ data vectors to each of $K$ cluster centers. This takes quite a toll on the computing power, so there are a few alternatives to mediate this problem. (Refer to PRML p.427)</li>
<li>Euclidean distance might not be applicable to some data types. In that case, we can come up with some other dissimilarity measure (a sort of metric) such as $V(\mathbf{x, x&rsquo;})$ and substitute into $J$.</li>
<li>K-means is a &ldquo;hard&rdquo; classifier as each data vector is assigned to exactly one cluster. But in practice, many data vectors lie in between cluster centers, so we might want to assign probabilities of assignment. Such is a motivation behind a &ldquo;soft&rdquo; classifier which we would see about shortly.</li>
</ol>
<h2 id="brk-means-in-r"><!-- raw HTML omitted -->K-means in R</h2>
<p>We would use faithful data in R base with some noise added for illustration.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(ggplot2)
<span style="color:#a6e22e">library</span>(gridExtra)
<span style="color:#a6e22e">library</span>(cowplot)

nNoise <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
Noise <span style="color:#f92672">=</span> <span style="color:#a6e22e">apply</span>(faithful, <span style="color:#ae81ff">2</span>, <span style="color:#a6e22e">function</span>(x)
  <span style="color:#a6e22e">runif</span>(nNoise, min<span style="color:#f92672">=</span><span style="color:#a6e22e">min</span>(x)<span style="color:#ae81ff">-0.1</span>, max<span style="color:#f92672">=</span><span style="color:#a6e22e">max</span>(x)<span style="color:#ae81ff">+0.1</span>))
df <span style="color:#f92672">=</span> <span style="color:#a6e22e">rbind</span>(faithful, Noise)
df <span style="color:#f92672">=</span> <span style="color:#a6e22e">apply</span>(df, <span style="color:#ae81ff">2</span>, <span style="color:#a6e22e">function</span>(x){<span style="color:#a6e22e">return</span>((x<span style="color:#f92672">-</span><span style="color:#a6e22e">mean</span>(x))<span style="color:#f92672">/</span><span style="color:#a6e22e">sd</span>(x))})
df <span style="color:#f92672">=</span> <span style="color:#a6e22e">data.matrix</span>(df)

<span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">data.frame</span>(df), <span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>eruptions, y<span style="color:#f92672">=</span>waiting))<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_point</span>()<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">theme_cowplot</span>()
</code></pre></div><p><img src="/image/BayesMLweek5/Rplot02.png" alt=""></p>
<p>K-means algorithm in R can be implemented with the following code;</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">hun_kmeans <span style="color:#f92672">=</span> <span style="color:#a6e22e">function</span>(df, n_K, tol<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>){
  
  vectornorm <span style="color:#f92672">=</span> <span style="color:#a6e22e">function</span>(x){<span style="color:#a6e22e">return</span>(<span style="color:#a6e22e">sqrt</span>(<span style="color:#a6e22e">sum</span>(x^2)))}
  
  df <span style="color:#f92672">=</span> <span style="color:#a6e22e">data.matrix</span>(df)
  
  <span style="color:#75715e">## initialize centroids ##</span>
  sampled_idx <span style="color:#f92672">=</span> <span style="color:#a6e22e">sample</span>(<span style="color:#a6e22e">nrow</span>(df), n_K)
  mu <span style="color:#f92672">=</span> df[sampled_idx, ]
  
  label <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#ae81ff">1</span>, <span style="color:#a6e22e">nrow</span>(df))
  SSE <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
  convergence <span style="color:#f92672">=</span> F
  
  <span style="color:#a6e22e">while</span>(<span style="color:#f92672">!</span>convergence){
    
    <span style="color:#75715e">## 1. update label ##</span>
    distance <span style="color:#f92672">=</span> <span style="color:#a6e22e">matrix</span>(<span style="color:#ae81ff">0</span>, nrow<span style="color:#f92672">=</span><span style="color:#a6e22e">nrow</span>(df), ncol<span style="color:#f92672">=</span>n_K)
    <span style="color:#a6e22e">for</span>(k in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>n_K){
      diff <span style="color:#f92672">=</span> <span style="color:#a6e22e">sweep</span>(df, <span style="color:#ae81ff">2</span>, mu[k,], <span style="color:#e6db74">&#34;-&#34;</span>)
      diff_norm <span style="color:#f92672">=</span> <span style="color:#a6e22e">apply</span>(diff, <span style="color:#ae81ff">1</span>, vectornorm)
      distance[,k] <span style="color:#f92672">=</span> diff_norm
    }
    label <span style="color:#f92672">=</span> <span style="color:#a6e22e">apply</span>(distance, <span style="color:#ae81ff">1</span>, which.min)
    
    <span style="color:#75715e">## 2. update centroids ##</span>
    <span style="color:#a6e22e">for</span>(k in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>n_K){
      mu[k,] <span style="color:#f92672">=</span> <span style="color:#a6e22e">apply</span>(df[label<span style="color:#f92672">==</span>k,], <span style="color:#ae81ff">2</span>, mean)
    }
    
    <span style="color:#75715e">## (compute SSE after each update) ##</span>
    error <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#ae81ff">0</span>, <span style="color:#a6e22e">nrow</span>(df))
    <span style="color:#a6e22e">for</span>(k in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>n_K){
      error[label<span style="color:#f92672">==</span>k] <span style="color:#f92672">=</span> <span style="color:#a6e22e">apply</span>(df[label<span style="color:#f92672">==</span>k,]<span style="color:#f92672">-</span>mu[k,], <span style="color:#ae81ff">1</span>, vectornorm)
    }
    convergence <span style="color:#f92672">=</span> (<span style="color:#a6e22e">abs</span>(SSE <span style="color:#f92672">-</span> <span style="color:#a6e22e">sum</span>(error)) <span style="color:#f92672">&lt;</span> tol)
    SSE <span style="color:#f92672">=</span> <span style="color:#a6e22e">sum</span>(error)
  }
  
  result <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(label <span style="color:#f92672">=</span> label, mu <span style="color:#f92672">=</span> mu, SSE <span style="color:#f92672">=</span> SSE)
  <span style="color:#a6e22e">return</span>(result)
}
</code></pre></div><p>and the result would look like</p>
<p><img src="/image/BayesMLweek5/Rplot01.png" alt=""></p>
<p>We can see that after 4 iterations, the algorithm found its way into the center of clusters.</p>
<p>But the convergence is not unique. For k=5, we could have</p>
<p><img src="/image/BayesMLweek5/Rplot03.png" alt=""></p>
<p><img src="/image/BayesMLweek5/Rplot04.png" alt=""></p>
<h3 id="brreferences"><!-- raw HTML omitted -->References</h3>
<ol>
<li>Pattern Recognition and Machine Learning, Bishop, 2006</li>
</ol>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/k-means/" rel="tag">K-means</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/clustering/" rel="tag">Clustering</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="Kang Gyeonghun avatar" src="/mypic1.jpg" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About Kang Gyeonghun</span>
	</div>
	<div class="authorbox__description">
		I study statistics, machine learning, data science or whatever that concerns making inference on infinitie dimension from a limited sample in fintie dimension. This blog is an archive of my journey of study.
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bayesian-ml/week3/03-bayesian-hierarchical-modeling-and-applications/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Bayesian Hierarchical Modeling and its Applications</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bayesian-ml/week5/02-mixtures-of-gaussians-and-em/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mixtures of Gaussians and EM algorithm</p>
		</a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hun-learning94" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-25-variational-inference/">Variational Inference and Bayesian Gaussian Mixture Model</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-24-forward-and-reverse-kl-divergence/">Forward and Reverse KL divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/bayesian-ml/week5/05-mle-minimizes-kl-divergence/">Interpretation of MLE in terms of KL divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/bayesian-ml/week5/04-note-on-kullback-leibler-divergence/">Note on Kullback-Leibler Divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-11-bayesian-networks-directed-acyclical-graphs/">Bayesian Networks (Directed Acyclical Graphs)</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/bayesian-ml/week5/03-em-algorithm-for-latent-variable-models/">EM Algorithm for Latent Variable Models</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/adaboost/" title="ADABOOST">ADABOOST</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/adaptive-basis-model/" title="Adaptive Basis Model">Adaptive Basis Model</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayes-rule/" title="Bayes Rule">Bayes Rule</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-gmm/" title="Bayesian GMM">Bayesian GMM</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-hierarchy/" title="Bayesian Hierarchy">Bayesian Hierarchy</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-networks/" title="Bayesian Networks">Bayesian Networks</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bootstrap/" title="Bootstrap">Bootstrap</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/cart/" title="CART">CART</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/clustering/" title="Clustering">Clustering</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/conjugacy/" title="Conjugacy">Conjugacy</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/d-seperation/" title="D-seperation">D-seperation</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/determinant/" title="Determinant">Determinant</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/diagonalization/" title="Diagonalization">Diagonalization</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/em-algorithm/" title="EM algorithm">EM algorithm</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ensemble-learning/" title="Ensemble Learning">Ensemble Learning</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/frequentist/" title="Frequentist">Frequentist</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gaussian-mixtures/" title="Gaussian Mixtures">Gaussian Mixtures</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/generalized-additive-models/" title="Generalized Additive Models">Generalized Additive Models</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gibbs-sampling/" title="Gibbs Sampling">Gibbs Sampling</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/intro-to-statistical-learning/" title="Intro to Statistical Learning">Intro to Statistical Learning</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/jacobian/" title="Jacobian">Jacobian</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/k-cv/" title="k-CV">k-CV</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/k-means/" title="K-means">K-means</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/kernel/" title="Kernel">Kernel</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/kl-divergence/" title="KL divergence">KL divergence</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lagrangian-duality/" title="Lagrangian Duality">Lagrangian Duality</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lasso/" title="Lasso">Lasso</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/latent-variable/" title="Latent Variable">Latent Variable</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lda/" title="LDA">LDA</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/linear-adjoint/" title="Linear Adjoint">Linear Adjoint</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/logistic-regression/" title="Logistic Regression">Logistic Regression</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/markov-chain/" title="Markov Chain">Markov Chain</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/matrix-derivatives/" title="Matrix Derivatives">Matrix Derivatives</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mcmc/" title="MCMC">MCMC</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/metropolis-hastings/" title="Metropolis Hastings">Metropolis Hastings</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mse/" title="MSE">MSE</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/multivariate-normal/" title="Multivariate Normal">Multivariate Normal</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/naive-bayes-classifier/" title="Naive Bayes Classifier">Naive Bayes Classifier</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ols/" title="OLS">OLS</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/pca/" title="PCA">PCA</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/posterior-approximation/" title="Posterior Approximation">Posterior Approximation</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/regression-splines/" title="Regression Splines">Regression Splines</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ridge/" title="Ridge">Ridge</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/self-adjoint/" title="Self Adjoint">Self Adjoint</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/similar-matrices/" title="Similar Matrices">Similar Matrices</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/singular-value-decomposition/" title="Singular Value Decomposition">Singular Value Decomposition</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/stochastic-process/" title="Stochastic Process">Stochastic Process</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/svm/" title="SVM">SVM</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/vector-derivatives/" title="vector derivatives">vector derivatives</a>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2020 Kang Gyeonghun.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>