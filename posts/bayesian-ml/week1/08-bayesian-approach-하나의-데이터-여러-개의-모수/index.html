<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Bayesian Approach: 하나의 데이터, 임의의 모수 - Hun Learning</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="Example article description">
		<meta property="og:title" content="Bayesian Approach: 하나의 데이터, 임의의 모수" />
<meta property="og:description" content="Example article description" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/bayesian-ml/week1/08-bayesian-approach-%ED%95%98%EB%82%98%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%97%AC%EB%9F%AC-%EA%B0%9C%EC%9D%98-%EB%AA%A8%EC%88%98/" />
<meta property="article:published_time" content="2020-07-20T05:30:00+09:00" />
<meta property="article:modified_time" content="2020-07-20T05:30:00+09:00" />

		<meta itemprop="name" content="Bayesian Approach: 하나의 데이터, 임의의 모수">
<meta itemprop="description" content="Example article description">
<meta itemprop="datePublished" content="2020-07-20T05:30:00+09:00" />
<meta itemprop="dateModified" content="2020-07-20T05:30:00+09:00" />
<meta itemprop="wordCount" content="804">



<meta itemprop="keywords" content="Bayes Rule," />

	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed">
		<a class="logo__link" href="/" title="Hun Learning" rel="home">
			<div class="logo__item logo__imagebox">
					<img class="logo__img" src="/mypic2.jpg">
				</div><div class="logo__item logo__text">
					<div class="logo__title">Hun Learning</div>
					<div class="logo__tagline">In Search Of The Truth Projected Onto A Finite Dimension</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/posts/about/">
				
				<span class="menu__text">Author</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
	<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
</script>
<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "all" } }
  });
</script>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Bayesian Approach: 하나의 데이터, 임의의 모수</h1>
			<div class="post__meta meta"><div class="meta__item-author meta__item">
	<svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2 0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class="meta__text">Kang Gyeonghun</span>
</div>
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2020-07-20T05:30:00&#43;09:00">2020-07-20</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/probabilistic-machine-learning/" rel="category">Probabilistic Machine Learning</a>
	</span>
</div></div>
		</header>
		<figure class="post__thumbnail">
			<img src="/cover/robotbayesian.jpg" alt="Bayesian Approach: 하나의 데이터, 임의의 모수">
		</figure><div class="content post__content clearfix">
			<h2 id="br0-생각하는-로봇은-베이지안이다"><!-- raw HTML omitted -->0. 생각하는 로봇은 베이지안이다!</h2>
<p>주변 환경을 인지하고 목적지를 찾는 로봇을 생각해보자. 목적지로 가는 경로에는 수많은 경우의 수가 있다. 이 경로에서 로봇은 시시각각 환경을 파악해서, 즉 데이터를 수집해서 가장 안전한 길을 택해야 한다. 전방에 위험징후를 포착했다. 로봇은 그 방향으로 가는 길이 위험하다고 판단해 경로를 변경해야 한다. 자 그러면 이걸 어떻게 코딩할까? 각각의 길이 위험할 확률 $p(road_i=unsafe)$과, 각각의 길에서 위험한 징후가 포착될 확률 $p(sign\mid road_i=unsafe)$ 을 고려하여, 위험할 확률 $p(road_i = unsafe \mid sign)$ 을 다시 계산해야한다. 사실 우리는 머릿 속으로 끊임없이 베이즈 정리를 쓰고 있지만 우리가 모르거나, 계산이 이상하거나, 착각을 하고 있을 뿐이다. 숫자로 생각하는 로봇은 철저한 베이지안이다!</p>
<h2 id="br1-bayes-rule-inverse-probability"><!-- raw HTML omitted -->1. Bayes Rule: &ldquo;Inverse&rdquo; Probability</h2>
<p><!-- raw HTML omitted -->먼저 베이즈 정리에 대해 간략히 소개하고, 이게 왜 혁신적인 발상의 전환인지 느껴보자.</p>
<p>베이즈 정리 자체는 product rule과 조건부 분포의 정의를 알면 바로 나온다.</p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
p(C|E) = \dfrac{p(C, E)}{p(E)} = \dfrac{p(C)p(E|C)}{p(E)} = \dfrac{p(C)p(E|C)}{\sum_{C&rsquo;}p(C&rsquo;)p(E|C&rsquo;)}
\end{align}
$$
<!-- raw HTML omitted -->여기서 분모의 $p(E)$ 개별 $C$에 의존하지 않는 상수이다. 때문에 다음과 같이 쓰기도 한다.</p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
p(C|E) \propto p(C)p(E|C)
\end{align}
$$
<!-- raw HTML omitted -->어떤 사건 E가 발생했을 때 그 원인이 C일 확률은, 애초에 C가 발생활 확률과, 그 C가 발생했을 때 E의 확률의 곱에 비례한다는 것. 만일 $p(C)$만 알고 있으면 <strong>&ldquo;사건 발생 후 원인의 확률을 묻는 문제&quot;가 &ldquo;원인이 주어졌을 때 사건의 확률를 묻는 문제&quot;로 바뀐다는 것.</strong> 인과관계가 역전된 것이 보이나? 이 때문에 이를 Inverse Probability라고도 한다.</p>
<p><!-- raw HTML omitted -->이걸 가설 $\mathcal{H}$에 한번 적용해볼까? 모수공간의 분할 $\mathcal{H}_1$, $\mathcal{H}_2$, $\mathcal{H}_3$, &hellip; 을 생각해보자. 그러면 $D$가 주어지면 다음과 같이 쓸 수 있다.</p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
p(\mathcal{H}|D) = \dfrac{p(D|\mathcal{H})p(\mathcal{H})}{p(D)}
\end{align}
$$</p>
<ul>
<li>$p(\mathcal{H})$: 데이터를 보기 전에 모든 가설이 가지는 확률</li>
<li>$p(D\mid\mathcal{H})$: 각각의 가설에서 데이터가 얼마나 Likely한지. 즉 데이터가 제공하는 가설의 Evidence</li>
<li>$p(\mathcal{H}\mid D)$: 데이터를 바탕으로 업데이트된, 모든 가설 각각에 대한 확률</li>
<li>$p(D)$: 모든 가능한 가설에서 주어진 데이터가 발생할 확률</li>
</ul>
<p><!-- raw HTML omitted -->여기에서 논란이 되는게 $p(\mathcal{H})$, 즉 prior이다. prior가 존재하지 않으니 인위적으로 prior를 만들면 어떻게든 나의 주관이 들어가게 되는거고, 그 순간 확률보다는 분포로 나타낸 가설에 대한 믿음이라는 해석이 더 적절하다. 그럼에도 불구하고 prior를 만들어서 계속 베이즈 정리를 쓰겠다는 것이 베이지안이고, 데이터를 보기 이전에 인위적으로 prior를 만든다는 발상 자체에 거부감을 느껴, &ldquo;어떻게 하면 prior가 없이 통계 분석을 할 수 있을까&quot;라는 물음에서 시작되어 Likelihood만 쓰는 방법이 빈도주의이다. (Reference 5)</p>
<h2 id="br2-베이즈-세계관-이해하기"><!-- raw HTML omitted -->2. 베이즈 세계관 이해하기</h2>
<p><!-- raw HTML omitted -->이제 아까 빈도론적 추론에서 본 것과 똑같은 세팅을 가져오자.</p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
\text{Data (iid):}\quad &amp;D = [x_1, x_2, &hellip;, x_n]\\\<br>
\text{Sampling Density of $D$:}\quad&amp; f(D|\theta) = \prod_{i=1}^nf(x_i|\theta) \quad (x_i\in \mathcal{X}, \theta \in \Omega)\\\<br>
\text{Likelihood of $\theta$:}\quad&amp; L(\theta|D)
\end{align}
$$
<!-- raw HTML omitted -->베이즈 추론에서 모수 $\theta$는 &ldquo;unknown thus uncertain&rdquo;. 즉 모르니까 확률변수다.  $p(\theta)$는 데이터 $D$를 보기 전(prior) 나의 믿음이고, $p(\theta|D)$는 데이터를 보고 난 후(posterior)의 나의 믿음이다. 그렇다면 $p(\theta|D)$ 를 어떻게 얻는가? 베이즈 정리를 사용한다. **베이지안은 모든 통계분석을 베이즈정리로 한다. 베이즈정리가 알파이자 오메가이다!**</p>
<!-- raw HTML omitted -->
<p>$$
\begin{align}
\text{(Prior) Belief in $\theta$} \quad &amp; p(\theta) \\\<br>
\text{Likelihood of $D$ at each $\theta$} \quad &amp; L(\theta|D)=p(D|\theta) \\\<br>
\text{Updated (Posterior) Belief in $\theta$} \quad &amp; p(\theta | x ) = \dfrac{p(D|\theta)p(\theta)}{\int_{\theta} p(D|\theta)p(\theta) d\theta}\quad\\\<br>
\end{align}
$$
<!-- raw HTML omitted -->빈도론 추론과 가장 큰 차이점은 $\theta$에 대한 추론의 결과를 분포로 제시한다는 것이다.</p>
<p><!-- raw HTML omitted -->새로운 데이터 $\tilde{x}$에 대한 빈도론적 예측 분포는 다음과 같다.
$$
\tilde{x} \sim p(x_i|\hat{\theta})
$$
<!-- raw HTML omitted -->새로운 데이터 $\tilde{x}$에 대한 베이지안 예측 분포는 다음과 같다.
$$
\begin{align}
\text{Prior Predictive:}\quad&amp; p(\tilde{x}) = \int p(x_i|\theta)p(\theta)d\theta\\\<br>
\text{Posterior Predictive:}\quad&amp; p(\tilde{x}) = \int p(x_i|\theta, D)p(\theta|D)d\theta\<br>
\end{align}
$$</p>
<!-- raw HTML omitted -->
<p>Posterior Predictive를 잘 살펴보면, 새로운 데이터의 확률은, 모든 가능한 경우 $\theta$에 걸쳐 그 데이터의 확률(likelihood) $p(x \mid \theta, D))$을 모두 고려하되, 그 각각의 가능성에 대해 사후믿음 $p(\theta \mid D)$만큼의 가중치를 주어서 평균한 값이라는 말이다. 빈도론에서의 예측 분포는 오직  $p(x \mid \theta_{mle})$이다. <!-- raw HTML omitted -->빈도론이 단 하나의 경우만을 고려한다면, 베이지안에서는 나의 믿음에 따라 모든 가능한 경우의 수를 모두 고려한다는 점이 가장 큰 차이점이다.<!-- raw HTML omitted --></p>
<h2 id="br3-베이지안-논리의-직관적인-이해"><!-- raw HTML omitted -->3. 베이지안 논리의 직관적인 이해</h2>
<p>이 부분은 이전에 만들어놓은 강의자료로 대체합니다!</p>
<p><img src="/image/CH01-17.png" alt="CH01-17"></p>
<p><img src="/image/CH01-18.png" alt="CH01-18"></p>
<p><img src="/image/CH01-19.png" alt="CH01-19"></p>
<p><img src="/image/CH01-20.png" alt="CH01-20"></p>
<p><img src="/image/CH01-21.png" alt="CH01-21"></p>
<p><img src="/image/CH01-22.png" alt="CH01-22"></p>
<p><img src="/image/CH01-23.png" alt="CH01-23"></p>
<p><img src="/image/CH01-24.png" alt="CH01-24"></p>
<p><img src="/image/CH01-25.png" alt="CH01-25"></p>
<h3 id="예시-지금-신촌에-비가-올까">예시: 지금 신촌에 비가 올까?</h3>
<p><strong><!-- raw HTML omitted -->(미국 Facebook 면접문제 변용) 오늘 아침 일기 예보를 보니 신촌에 비가 올 확률이 $25%$이라고 합니다. 아침을 먹고 현관문을 나서기 전, 당신은 자취하는 친구 세 명에게 지금 비가 오는지 따로따로 물어봅니다. 모두 지금 비가 온다고 합니다. 하지만 당신의 친구들은 당신을 곯리기 위해 세 번 중 한 번은 거짓말을 합니다. 그렇다면 당신은 우산을 챙길 것인가요? 그 이유와 함께 설명해주세요.</strong></p>
<p><!-- raw HTML omitted -->인터넷에 떠도는 페이스북 면접 문제를 가져와서 변용하였다. 이 문제에서 친구들의 답변 $YYY$를 Data, 비가 내리는 여부를 모수 $\theta$로 생각하면, 다음과 같은 두 풀이가 가능하다.</p>
<!-- raw HTML omitted -->
<ol>
<li>
<p><strong>MLE 접근</strong>. 비가 내려는 분포에서 YYY의 확률은 $p(YYY\mid rain) = (2/3)^3 = 8/27$, 비가 내리지 않는 분포에서 YYY의 확률은 $p(YYY\mid rain) = (1/3)^3 = 1/27$. 비가 내릴 때의 표본의 Likelihood가 더 크니 우산을 챙긴다.
<!-- raw HTML omitted --></p>
</li>
<li>
<p><strong>Bayes Rule 접근</strong>. 비가 내리는 사건의 사전 확률(믿음)은 $0.25$이다. 데이터를 바탕으로 이 믿음을 업데이트하면
<!-- raw HTML omitted -->
$$
\begin{align}
p(Rain|YYY) &amp;= \dfrac{p(YYY|Rain)p(Rain)}{p(YYY|Rain)p(Rain) + p(YYY|NoRain)p(NoRain)}\\\<br>
&amp;=\dfrac{8/27 \times 1/4}{8/27 \times 1/4 + 1/27 \times 3/4}\\\<br>
&amp;= \dfrac{8 \times 1}{8 \times 1 + 1 \times 3} = 8/11 &gt; 0.25
\end{align}
$$
<!-- raw HTML omitted -->즉 친구들의 대답으로 인해 나의 믿음이 $0.25$에서 $0.72$로 올라갔으니 우산을 챙긴다는 것.</p>
</li>
</ol>
<p><!-- raw HTML omitted -->결론은 똑같지만 사고 과정이 다르다.</p>
<h2 id="brreferences"><!-- raw HTML omitted -->References</h2>
<ol>
<li>Probability Theory and Statistical Inference: Econometric Modeling with Observational Data (Spanos, 1999)</li>
<li>Machine Learning: a Probabilistic Perspective (Murphy, 2012)</li>
<li>Computer Age Statistical Inference (Efron, Hastie, 2016)</li>
<li>Calibration of p Values for Testing Precise Null Hypotheses (Sellke et al, 2001)</li>
<li><a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf">https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf</a></li>
</ol>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/bayes-rule/" rel="tag">Bayes Rule</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="Kang Gyeonghun avatar" src="/mypic1.jpg" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About Kang Gyeonghun</span>
	</div>
	<div class="authorbox__description">
		I study statistics, machine learning, data science or whatever that concerns making inference on infinitie dimension from a limited sample in fintie dimension. This blog is an archive of my journey of study.
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bayesian-ml/week1/07-%EB%B9%88%EB%8F%84%EB%A1%A0%EC%A0%81-%EA%B7%80%EB%AC%B4%EA%B0%80%EC%84%A4%EC%9C%A0%EC%9D%98%EC%88%98%EC%A4%80%EA%B2%80%EC%A0%95nhst%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">07 빈도론적 귀무가설유의수준검정(NHST)의 문제점</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bayesian-ml/week2/01-conjugate-prior-univariate-poisson/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Conjugate Prior for Univariate - Poisson Model</p>
		</a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hun-learning94" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/posts/how-to-add-rtools-to-windows-path-env/">Rtools를 윈도우 환경변수 PATH에 추가하는 방법</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-25-variational-inference/">Variational Inference and Bayesian Gaussian Mixture Model</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-24-forward-and-reverse-kl-divergence/">Forward and Reverse KL divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/bayesian-ml/week5/05-mle-minimizes-kl-divergence/">Interpretation of MLE in terms of KL divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/bayesian-ml/week5/04-note-on-kullback-leibler-divergence/">Note on Kullback-Leibler Divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-11-bayesian-networks-directed-acyclical-graphs/">Bayesian Networks (Directed Acyclical Graphs)</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/adaboost/" title="ADABOOST">ADABOOST</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/adaptive-basis-model/" title="Adaptive Basis Model">Adaptive Basis Model</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayes-rule/" title="Bayes Rule">Bayes Rule</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-gmm/" title="Bayesian GMM">Bayesian GMM</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-hierarchy/" title="Bayesian Hierarchy">Bayesian Hierarchy</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-networks/" title="Bayesian Networks">Bayesian Networks</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bootstrap/" title="Bootstrap">Bootstrap</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/cart/" title="CART">CART</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/clustering/" title="Clustering">Clustering</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/conjugacy/" title="Conjugacy">Conjugacy</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/d-seperation/" title="D-seperation">D-seperation</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/determinant/" title="Determinant">Determinant</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/diagonalization/" title="Diagonalization">Diagonalization</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/em-algorithm/" title="EM algorithm">EM algorithm</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ensemble-learning/" title="Ensemble Learning">Ensemble Learning</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/frequentist/" title="Frequentist">Frequentist</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gaussian-mixtures/" title="Gaussian Mixtures">Gaussian Mixtures</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/generalized-additive-models/" title="Generalized Additive Models">Generalized Additive Models</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gibbs-sampling/" title="Gibbs Sampling">Gibbs Sampling</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/intro-to-statistical-learning/" title="Intro to Statistical Learning">Intro to Statistical Learning</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/jacobian/" title="Jacobian">Jacobian</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/k-cv/" title="k-CV">k-CV</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/k-means/" title="K-means">K-means</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/kernel/" title="Kernel">Kernel</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/kl-divergence/" title="KL divergence">KL divergence</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lagrangian-duality/" title="Lagrangian Duality">Lagrangian Duality</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lasso/" title="Lasso">Lasso</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/latent-variable/" title="Latent Variable">Latent Variable</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lda/" title="LDA">LDA</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/linear-adjoint/" title="Linear Adjoint">Linear Adjoint</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/logistic-regression/" title="Logistic Regression">Logistic Regression</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/markov-chain/" title="Markov Chain">Markov Chain</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/matrix-derivatives/" title="Matrix Derivatives">Matrix Derivatives</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mcmc/" title="MCMC">MCMC</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/metropolis-hastings/" title="Metropolis Hastings">Metropolis Hastings</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mse/" title="MSE">MSE</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/multivariate-normal/" title="Multivariate Normal">Multivariate Normal</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/naive-bayes-classifier/" title="Naive Bayes Classifier">Naive Bayes Classifier</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ols/" title="OLS">OLS</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/pca/" title="PCA">PCA</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/posterior-approximation/" title="Posterior Approximation">Posterior Approximation</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/rcpp/" title="Rcpp">Rcpp</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/regression-splines/" title="Regression Splines">Regression Splines</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ridge/" title="Ridge">Ridge</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/self-adjoint/" title="Self Adjoint">Self Adjoint</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/similar-matrices/" title="Similar Matrices">Similar Matrices</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/singular-value-decomposition/" title="Singular Value Decomposition">Singular Value Decomposition</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/stochastic-process/" title="Stochastic Process">Stochastic Process</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/svm/" title="SVM">SVM</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/vector-derivatives/" title="vector derivatives">vector derivatives</a>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2020 Kang Gyeonghun.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>