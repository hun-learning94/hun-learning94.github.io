<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Bayesian Modelling by Zoubin Ghahramani, MLSS2012, Univ of Cambridge - Hun Learning</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="Example article description">
		<meta property="og:title" content="Bayesian Modelling by Zoubin Ghahramani, MLSS2012, Univ of Cambridge" />
<meta property="og:description" content="Example article description" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/2020-07-31-bayesian-modeling/" />
<meta property="article:published_time" content="2020-07-31T09:10:00+09:00" />
<meta property="article:modified_time" content="2020-07-31T09:10:00+09:00" />

		<meta itemprop="name" content="Bayesian Modelling by Zoubin Ghahramani, MLSS2012, Univ of Cambridge">
<meta itemprop="description" content="Example article description">
<meta itemprop="datePublished" content="2020-07-31T09:10:00+09:00" />
<meta itemprop="dateModified" content="2020-07-31T09:10:00+09:00" />
<meta itemprop="wordCount" content="2044">



<meta itemprop="keywords" content="Posterior Approximation," />

	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed">
		<a class="logo__link" href="/" title="Hun Learning" rel="home">
			<div class="logo__item logo__imagebox">
					<img class="logo__img" src="/mypic2.jpg">
				</div><div class="logo__item logo__text">
					<div class="logo__title">Hun Learning</div>
					<div class="logo__tagline">In Search Of The Truth Projected Onto A Finite Dimension</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/posts/about/">
				
				<span class="menu__text">Author</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
	<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
</script>
<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "all" } }
  });
</script>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Bayesian Modelling by Zoubin Ghahramani, MLSS2012, Univ of Cambridge</h1>
			<p class="post__lead">2012 영국 케임브리지 대학교 여름방학 머신러닝 강의 요약</p>
			<div class="post__meta meta"><div class="meta__item-author meta__item">
	<svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2 0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class="meta__text">Kang Gyeonghun</span>
</div>
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2020-07-31T09:10:00&#43;09:00">2020-07-31</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/probabilistic-machine-learning/" rel="category">Probabilistic Machine Learning</a>
	</span>
</div></div>
		</header>
		<figure class="post__thumbnail">
			<img src="/posts/mlss2012_ghahramani_bayesian_modelling/BayesModelLecture.JPG" alt="Bayesian Modelling by Zoubin Ghahramani, MLSS2012, Univ of Cambridge">
		</figure><div class="content post__content clearfix">
			<p>베이지안 머신러닝에 대해 인터넷에서 자료를 찾다보니 꽤 괜찮은 동영상 강의가 있어서 요약해보았다. 베이지안 모델링에 대해 개괄적으로 설명해주는 강의인데, 머신러닝에서 베이즈 정리가 어떻게 쓰이는지 잘 설명된 자료인 것 같다.</p>
<p><a href="http://videolectures.net/mlss2012_ghahramani_bayesian_modelling/">http://videolectures.net/mlss2012_ghahramani_bayesian_modelling/</a></p>
<p>위 링크에서 해당 강의 자료를 다운받고 시청할 수 있다. 다만 어도비 플래시가 있어야 구동이 되니 아마 올해가 지나면 못 듣지 않을까 싶다. 베이지안 모델링 외에도 Bayesian Nonparametrics, Graphical Model 등등 다른 다양한 강의가 있으니 한번 참고해보자.</p>
<p>아래에다가 강의 슬라이드별로 강의에서 아저씨가 말씀하신 부분을 나름 보충을 섞어 요약해놨다. 그래도 강의를 직접 보는 편이 도움이 많이 될 것.</p>
<h3 id="br강의-내용-요약"><!-- raw HTML omitted -->강의 내용 요약</h3>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-01.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-02.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-03.jpg" alt="1"></p>
<ul>
<li>데이터에 대한 모델이 만족해야될 요건은, 일단 데이터에 내재한 불확실성을 잘 반영해야 함. 그 불확실성은 1) 데이터 자체의 노이즈와, 2) 진짜 모델과 모수에 대해 잘 모르기 때문에 발생함.</li>
<li>데이터의 크기와 변동에 대해서 Robust 해야 한다.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-04.jpg" alt="1"></p>
<ul>
<li>모델이란 &ldquo;a description of possible data one could observe from a system&rdquo;
즉 실제 데이터 형성 과정에 대한 설명이 바로 모델이다.</li>
<li>실제 데이터 형성 과정을 잘 반영한다는 것은 예측 결과가 실제와 잘 맞아야한다는 것이다.</li>
<li>미적분이 &ldquo;rate of change&quot;에 대해 설명하는 수학적 언어였다면 확률은 &ldquo;uncertainty&quot;에 대해 이야기하는 수학적 언어이다. 때문에 자연스럽게 데이터에 내재된 불확실성을 잘 반영하기 위해서는 확률 모델을 쓰는 것이다.</li>
<li>확률 모델을 쓰기 때문에 우리는 베이즈 정리를 사용해 데이터에서 관측되지 않는 미지의 양에 대한 추론을 하고, 새로운 관측치에 대한 예측도 하고, 데이터에서 배울 수 있는 것.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-05.jpg" alt="1"></p>
<ul>
<li>베이즈 정리가 대단한 이유는
<ul>
<li>데이터를 보기 전에 대한 가설에 대한 믿음과</li>
<li>데이터를 보고 난 후의 가설에 대한 믿음을 매끄럽게 연결해주기 때문</li>
</ul>
</li>
<li>이 연결고리가 바로 Likelihood이다. Likelihood의 역할은 어떤 가설에 대하여 데이터에 확률을 부여하는 것.
<ul>
<li>때문에 가설이 &ldquo;well formulated&quot;되기 위한 요건은 바로 데이터에 확률을 부여할 수 있어야 한다.</li>
<li>긍까 존나 자세해야 된다는 거임. 그냥 y와 x의 관계는 선형이다! 하면 안 되고 베타는 -1부터 1까지의 유니폼 분포를 따르고, 오차항은 가우시안이고, 등등 가정을 계속 써나가다 보면 어떤 순간 Likelihood가 탄생한다.</li>
<li>즉 Likelihood는 데이터에 대한 가정이, 데이터에 대해 확률을 부여할 수 있을 정도로 자세하게 정의된 것.</li>
</ul>
</li>
<li>베이지안 모델러들은 Likelihood와 Prior를 같이 먼저 정해야한다. 이게 바로 베이지안 모델임.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-06.jpg" alt="1"></p>
<ul>
<li>머신러닝의 목적을 크게 두 가지 보자면
<ol>
<li>데이터의 형성 과정, 즉 모델을 배우기 위해 머신러닝 알고리즘을 사용</li>
<li>일단 데이터 다운 받고 모든 알고리즘을 다 돌려본 다음에 가장 잘 나온 거 가지고 예측하고 결정하고&hellip; 그냥 다 때려넣어보는 것. 굳이 모델을 찾겠다는 마인드가 아님</li>
</ol>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-07.jpg" alt="1"></p>
<ul>
<li>목차..</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-08.jpg" alt="1"></p>
<ul>
<li>자세한 목차&hellip; 많이 스킵할거임</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-09.jpg" alt="1"></p>
<ul>
<li>머신러닝의 대표적인 예시를 가지고 설명해보자. 머신러닝에서 확률적인 접근이란 데이터의 형성 과정에 대한 Likelihood 함수를 세우는 것을 의미한다.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-10.jpg" alt="1"></p>
<ul>
<li>Classification 문제에서 데이터 $x$가 주어졌을 때 $y$가 $-1$ 또는 $1$일 확률을 Likelihood 함수로 나타낼 수 있다.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-11.jpg" alt="1"></p>
<ul>
<li>
<p>고차항 회귀분석에서 오차항의 정규분포를 가정하면 likelihood 함수를 쓸 수 있다. 여기에서 모수에 대해 Prior 믿음을 주고 베이즈 정리로 업데이트를 하면 그것이 Bayesian Linear Regression이 되는 것이다.</p>
</li>
<li>
<p>Bayesian Linear Regression이 잘 설명된 그림이 아래 그림이다. (<a href="http://krasserm.github.io/2019/02/23/bayesian-linear-regression/">출처</a>)</p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/BayesLinearRegression.png" alt="BLR"></p>
<p>맨 왼쪽 열이 parameter space에 그려진 모수에 대한 믿음의 분포이며, 이 분포에서 추출한 모수(회귀선)이 그려진 것이 가운데 그림이고, 이에 대응하는 posterior predictive 분포가 제일 오른쪽에 있는 열이다.</p>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-12.jpg" alt="1"></p>
<ul>
<li>
<p>Clustering도 마찬가지로 GMM으로 Likelihood를 세우고 EM으로 fitting했었다. 블로그에 <a href="https://hun-learning94.github.io/posts/bayesian-ml/week5/02-mixtures-of-gaussians-and-em/">GMM</a>과 <a href="https://hun-learning94.github.io/posts/bayesian-ml/week5/03-em-algorithm-for-latent-variable-models/">EM</a> 포스팅 참조.</p>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-13.jpg" alt="1"></p>
<ul>
<li>Likelihood 나왔지? 베이즈 룰 써라. 끝.
<ul>
<li>솔직히 베이즈 룰이라고 거창하게 말할 것도 없이 그냥 Sum rule하고 Product rule만 알면 끝이다.</li>
</ul>
</li>
<li>prediction? average 때려라!</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-14.jpg" alt="1"></p>
<ul>
<li>는 훼이크!</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-15.jpg" alt="1"></p>
<ul>
<li>왜 베이지안이냐? 아니.. 뭐 미적분 쓰면 다 미적부니안인가.. 야 그냥 확률모델 세우니까 당연히 썸룰이랑 프로덕트 룰 쓰는거임&hellip; 굳이 베이지안이라고 부르는게 이상함;;</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-16.jpg" alt="1"></p>
<ul>
<li>정말 당연하다는 거를 느껴보자. 니가 로봇 만든다고 하자. 알고리즘 짤 때 저런거 다 고려해야함. 그럼 베이즈 룰 써야지. ㅇㅋ?</li>
<li>즉 여러 가설에 대한 믿음을 수치적으로 나타내려면 확률을 쓰는 거고, 데이터를 보기 전과 본 후의 확률의 변화를 계산하려면 베이즈 룰을 쓸 수 밖에 없다는 것.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-17.jpg" alt="1"></p>
<ul>
<li>믿음이 왜 확률이 되는지 자세한 논의. Robot&rsquo;s belief이 저런 조건들을 다 만족하면 그 belief가 결국 확률의 3공리를 만족해야 한다는 거임. 그래서 베이즈 룰 가능</li>
<li>반대로 말하면 확률론이란 합리적인 의사결정주체(rational agent)의 믿음 체계를 나타내는 언어라는 것. &ldquo;It extends logic to strength of belief&rdquo;</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-18.jpg" alt="1"></p>
<ul>
<li>Model of economic rationality: economic case for the use of probability to represent belief</li>
<li>믿음이 0.9이면 나는 9to1 odd 혹은 그보다 더 좋은 조건을 받아들일 준비가 됐다는 거임</li>
<li>뭔 소리임? 그냥 이전의 슬라이드 내용을 다른 방식으로 보인 거임</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-19.jpg" alt="1"></p>
<ul>
<li>Theoretical aspect of bayesian methods</li>
<li>Bayesian이 꼭 모수를 랜덤변수라고 보는 건 아님. 참 모수의 존재를 인정해도 전혀 문제 없음.</li>
<li>(어떤 정규성 가정을 만족하면) a priori하게 참 모수 근처에 어떤 확률 mass라도 있으면 데이터가 무한으로 갈수록 우리의 사후믿음은 참모수만 1인 디락 델타 펑션이 수렴할거임</li>
<li>우리 prior에 없는 모델이 진짜 답이라도 결국 포스테리어는 실제와 가장 잘 근사한 놈을 집어냄</li>
<li>MLE와의 차이점은 MLE는 추정량이라는 하나의 값이 참 모수로 확률수렴하다고 얘기한 거라면, 이 정리는 Posterior라는 확률분포 자체가 하나의 분포로 분포수렴한다고 얘기하는 것이다.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-20.jpg" alt="1"></p>
<ul>
<li>똑같은 곳으로 수렴하니 둘의 차이는 어심토틸하게 0이 된다.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-21.jpg" alt="1"></p>
<ul>
<li>underfitting과 overfitting을 피하기 위한 베이지안 모델 설렉션을 알아보자.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-22.jpg" alt="1"></p>
<ul>
<li>데이터 $\mathcal{D}$가 주어진 상태에서 Model evidence $p(\mathcal{D}\mid m)$를 어떻게 계산할 것인가? analytic하게 계산할 수 있으면 제일 좋지. 근데 그게 안 되면&hellip;
<ul>
<li>일단 prior $p(\boldsymbol{\theta}\mid m)$에서 $\boldsymbol{\theta}_1$를 랜덤 샘플한 후에 $p(\mathcal{D}\mid \boldsymbol{\theta}_1, m)$를 계산하고, 그걸 한 10000번 해서 $(s=1:10000)$ $p(\mathcal{D}\mid \boldsymbol{\theta}_s, m)$의 평균을 구한다. (몬테카를로 적분)</li>
<li>그러나 데이터가 많아질수록 $p(\mathcal{D}\mid \boldsymbol{\theta}, m)$가 아주 그냥 지수적으로 뾰족해지기때문에 needle in a haystack같음.</li>
</ul>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-23.jpg" alt="1"></p>
<ul>
<li>베타계수 노말, 분산 인버스 감마, full conjugate prior이라거 posterior에서 초록색 애들 바로 뽑을 수 있음.</li>
<li>코딩으로 직접 보일 수 있다. 언젠가 만들어서 블로그 올릴거임&hellip;</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-24.jpg" alt="1"></p>
<ul>
<li>hyperparameter의 역할은 parameter를 하나로 묶는 것이지, 거기서 더 계단을 올라가봤자 의미가 없다.</li>
<li>hyperparameter에 넣을 값을 데이터를 보고 정하는게 empirical bayes</li>
<li>블로그에 Hierarchical Model에 대한 포스팅이 있으니 참조해보자.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-25.jpg" alt="1"></p>
<ul>
<li>prior는 우리의 믿음을 최대한 반영해야함. 그럼 어떻게 반영하냐? 도메인 지식이 중요하겠지.
<ul>
<li>그냥 겸손하게 not too ridiculous한 파라미터의 range를 가져오는 것도 belief라고 볼 수 있음. subjective prior라고 해서 크게 겁먹을 필요가 없음.</li>
</ul>
</li>
<li>그렇게 만든 prior를 어떻게 평가한가? prior에서 데이터를 만들어보고 그 결과를 함 보자. 데이터에 맞추는 게 아니라 상식적으로 그 자료의 성격에 맞아야 한다는 것. 예컨대 자료가 사람의 키인데 내가 만든 prior에는 키가 막 3m인 애들도 우수수 나오면 prior 설정이 잘못된거임</li>
<li>결국 prior가 어쨋든 간에 베이지안의 핵심은 averaging이니 크게 신경 노노!</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-26.jpg" alt="1"></p>
<ul>
<li>먼저 데이터를 보고 hyperparameter를 정한다는게 사실 컨닝하는 거라서, 막 내가 전혀 예상하지 못했던 데이터가 와도 모델이 데이터를 잘 설명하게 되는 장점이 있긴 함. 그래서 robust하다고도 하는데</li>
<li>그런데 데이터에 잘 맞는다는 것은 그만큼 overfitting하게 되는 거임. 데이터를 보고 hyperparameter를 정하고 또 그 모델을 data로 학습시키는 것이니 double counting</li>
<li>그러나 한 두 개 정도는 이렇게 해도 overfitting이 그렇게 심각하지는 않음</li>
<li>$\theta$의 MLE를 찾는게 Level 1 ML</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-27.jpg" alt="1"></p>
<ul>
<li>
<p>지수분포족의 덴시티에서 데이터와 파라미터의 관계는 지수항 안에서 선형으로 이뤄짐</p>
</li>
<li>
<p>$\phi$를 natural parameter, $s(x)$를 sufficient statistics이라고 함. 예컨대 Gaussian에서 natural parameter는 $\mu, \sigma^2$의 reparameterization
$$
\begin{align}
p(x\mid \mu, \sigma^2) &amp;\propto 
\exp{-\dfrac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}\<br>
&amp;\propto \exp{-\dfrac{1}{2\sigma^2}\sum_{i=1}^nx_i^2+\dfrac{\mu}{\sigma^2}\sum_{i=1}^n x_i }\<br>
&amp;= \exp{ 
\begin{bmatrix} -1/2\sigma^2 &amp; \mu/\sigma^2 \end{bmatrix}
\begin{bmatrix} \sum_{i=1}^nx_i^2 \ \sum_{i=1}^nx_i \end{bmatrix}}
\end{align}
$$</p>
</li>
<li>
<p>위에 $p(x\mid \theta)$는 $x$에 대한 pdf인데 $p(\theta)$는 $\theta$에 대한 pdf이지만 둘의 형태가 같음</p>
</li>
<li>
<p>때문에 conjugate prior는 posterior가 그냥 모수 hyperparameter만 업데이트하는거임
$$
\begin{align}
\eta &amp;\to \eta + N\<br>
\nu &amp;\to \nu+\sum_n s(x_n)
\end{align}
$$</p>
<ul>
<li>conjugate prior를 쓰는 이유: 간편하니까! 그리고 지수분포족 prior가 모수의 영역에 확률을 적당히 뿌리기에 좋음. 어차피 믿음만 제대로 표현할 수 있으면 되니까 이왕이면 편하게 conjugate한 애들 쓰는거임. 믿음을 있는 그대로 표현해야지 왜 지수분포족으로 타협하냐고? 아니 이거만 써도 믿음을 충분히 뿌릴 수 있다니까..?</li>
</ul>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-28.jpg" alt="1"></p>
<ul>
<li>이미 봤지요?</li>
<li>여기서 가장 구하기 어려운게 $p(\mathcal{D}\mid m)$ marginal likelihood, 우리가 모델을 $m$이라고 특정해서 그렇지 그냥 베이즈 룰에서 보는 $p(\mathcal{D})$를 말하는거임</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-29.jpg" alt="1"></p>
<ul>
<li>왜 어렵냐고?
<ol>
<li>$p(\mathbf{y}\mid m)$의 적분이 엄청 고차원인 경우가 많음. 모수 개수가 많을수록 모수 개수만큼 n중적분을 해줘야 함</li>
<li>심이저 latent variable $\mathbf{x}$의 존재를 가정하면 얘네들에 대해서도 적분을 해야함</li>
<li>애초에 Likelihood 자체가 계산이 너무 복잡한 경우도 많음</li>
</ol>
</li>
<li>때문에 이 marginal likelihood와 posterior를 우회하여 구하는 방법이 수십 년 동안 연구가 됨. 우리가 배울 내용들임</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-30.jpg" alt="1"></p>
<ul>
<li>optimization 문제의 해결을 위해 여러 알고리즘이 있는 것처럼 marginalization 문제의 해결을 위해서도 여러 알고리즘이 있음</li>
<li>각각의 알고리즘은 모두 speed - accuracy tradeoff 관게에 있다.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-31.jpg" alt="1"></p>
<ul>
<li>
<p>PRML에 자세히 나옴. 한 마디로 posterior를 MAP에서 정규근사하고 $p(\mathbf{y}) = \dfrac{p(\boldsymbol{\theta})p(\mathbf{y} \mid \boldsymbol{\theta}) }{p(\boldsymbol{\theta}\mid \mathbf{y})}$를 이용해 marginal likelihood에 넣는 것임</p>
</li>
<li>
<p>아까 posterior 분포는 $n\to \infty$에 따라 참 모수 $\boldsymbol{\theta}^{true}$에서의 디락 델타 펑션이 된다고 했는데, 그 수렴하는 과정이 정규분포라고 볼 수 있다고 해보자. 엄밀하지는 않음</p>
<ul>
<li>
<p>$n$이 크다는 것은 $\hat{\boldsymbol{\theta}}=\boldsymbol{\theta}^{MAP} \to \boldsymbol{\theta}^{true}$이니까 수렴하는 과정이라는 것. 때문에 posterior를 $\boldsymbol{\theta}^{MAP}$를 중심으로 한 정규분포로 근사할 수 있음</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>이 식의 근본이 뭐냐? 다음과 같이 어떤 pdf의 상수배만 알고 있을 때</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
</li>
<li>
<p>이때 postive matrix인 $A$가 symmetric definite하려면 (그래야 정규분포가 well-defined되니까) Hessian인 $\nabla\nabla f(\boldsymbol{\theta})\mid_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}}$의 모든 원소가 0보다 작아야 하며, 이는 즉 포스테리어에서 MAP가 saddle point같은 게 아니라 local maximum이어야 한다는 것을 의미함.</p>
<!-- raw HTML omitted -->
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/laplace.PNG" alt="1"></p>
<p>(출처: PRML, p.215)</p>
<!-- raw HTML omitted -->
</li>
</ul>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-32.jpg" alt="1"></p>
<ul>
<li>
<p>Laplace Approximation에서 계산이 빡센 부분은 행렬식을 구하는 것. $d\times d$ 행렬의 행렬식을 구하는 데에는 대략 $d^{3.2+o(1)}\approx d^3$번 연산이 필요. (<a href="https://repository.lib.ncsu.edu/bitstream/handle/1840.2/87/KaVi04_2697263.pdf?sequence=1&amp;isAllowed=y">참조</a>)</p>
</li>
<li>
<p>아 행렬식 구하는거도 힘들다. 그럼 BIC 쓰는거임</p>
<ul>
<li>Laplace Approximation의 식에서 $n$에 의존하지 않는 애들은 다 날려버리자. 그럼 프라이어, 상수항 다 날라감.</li>
<li>Likelihood $\ln p(\mathbf{y}\mid \hat{\boldsymbol{\theta}})$는 데이터 개수 $n$에 따라 linear하게 증가함. iid 샘플이면 $\ln p(\mathbf{y}\mid \hat{\boldsymbol{\theta}})$는 개별 데이터 $y_i$의 로그우도의 합이니까.</li>
<li>$-A=\nabla\nabla\ln p(\boldsymbol{\theta})p(\mathbf{y}\mid \boldsymbol{\theta})\mid _{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}}$도 자세히는 안 보이지만 $n$에 따라 linear하게 증가함. 그러면 $d \times d$행렬이니까 $\mid A\mid$는 $n^d$만큼 증가함. 그래서 로그 씌우면 대충 $\ln \mid A \mid \approx d\ln n$ as $n \to \infty$.</li>
</ul>
</li>
<li>
<p>prior에 의존하지 않으니까 MAP 대신에 MLE 써도 됨</p>
</li>
<li>
<p>Minimum Discription Length Criterion하고 똑같음. <em>그게 뭔데..</em></p>
</li>
<li>
<p>문제는 BIC 식에서 model complexity가 parameter의 개수로 표현이 되어있다는 건데, 모수의 개수와 model complexity가 항상 같이 가는 것은 아님. 모수가 적어도 엄청 복잡한 모델이 얼마든지 있을 수 있고, 간단한 모델이어도 모수가 무한대일 수 있음. 그래서 전반적으로 모수 개수로 모델 복잡도를 말하는 것은 안 좋음</p>
<ul>
<li>다만 모델이 identifiable하다 등의 여러 가정이 충족될 때 &ldquo;점근적으로는&rdquo; 위처럼 BIC로 model evidence를 근사하는 접근이 설득력이 있음.</li>
<li>identifiable하다는 조건은 극한에서 modal이 하나밖에 없어야 하는 것.</li>
</ul>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-33.jpg" alt="1"></p>
<ul>
<li>
<p>marginal likelihood를 바로 못 구하면 lower bound라도 구해보자. 그래서 lower bound끼리 대소를 비교해 모델을 비교해보자.</p>
</li>
<li>
<p>$\boldsymbol{x}$를 어떤 잠재변수라고 하자. 위에 부등식이 성립하는 이유는 Log가 concave하기 때문이다. 즉 평균의 로그값이 로그값의 평균보다 크기 때문. 젠센의 부등식에 의해</p>
<!-- raw HTML omitted -->
</li>
<li>
<p>만일 $q$가 간단해서 $\boldsymbol{x, \theta}$의 식으로 각각 깔끔하게 나눠질 수 있다면, $q_{\boldsymbol{\theta}}(\boldsymbol{\theta}), q_{\boldsymbol{x}}(\boldsymbol{x})$를 순차적으로 update하면서 $\mathcal{F}_m$의 값을 최대화하는 알고리즘을 생각할 수 있는데, 그게 바로 EM 알고리즘!</p>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-34.jpg" alt="1"></p>
<ul>
<li>Variational Bayes에 대한 자세한 내용은 PRML 10장을 공부할때 글을 써보겠다!</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-36.jpg" alt="1"></p>
<p><!-- raw HTML omitted --></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-37.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-38.jpg" alt="1"></p>
<ul>
<li>
<p>저번 학기에 다 본 내용이긴 한데 잠깐 복기하자면, $x$를 보고 $y \in {-1, 1}$를 결정하는 Binary Classification 문제에서 확률 모델 $p(y=1\mid x)$을 짜는 방법은 두 종류가 있음.</p>
<ul>
<li>Generative Model:  $p(y=1\mid x)=\dfrac{p(y=1)p(x \mid y=1)}{p(x)}$으로 봐서 $p(y)$와 $p(x\mid y)$을 추정하는 모델. Naive Bayes Classifier, LDA, QDA 등이 있음</li>
<li>Discriminative Model: $p(y=1\mid x)$을 바로 모델링하는 방법. 대표적인게 Logistic Regression.</li>
</ul>
</li>
<li>
<p>Generative Model은 베이즈 정리를 사용하긴 하나 베이지안 방법은 아니다. 베이지안 방법의 핵심은</p>
<ol>
<li>하나의 답을 내지 않고 가능한 모든 답의 분포를 보여주며</li>
<li>그 가능한 모든 답의 분포의 평균을 사용한다 (averaging) 는 것.</li>
</ol>
<p>그러나 예컨대 Naive Bayes Classifier같은 경우는 하나의 classifier만을 보여줄 뿐임.</p>
</li>
<li>
<p>베이지안 classifier의 예로는 Bayes Point Machines가 있음. 이를 SVM (Maximal Margin Classifier)하고 비교해보면</p>
<ul>
<li>SVM은 마진에 위치한 점들 (support vectors) 만을 고려하여 이 점들로부터 마진이 최대가 되는 하나의 hyperplane만을 제시함.</li>
<li>그러나 위의 그림처럼 (설명의 단순화를 위해 linear classifiable한 경우만 보자면) 두 클래스를 나누는 hyperplane은 회색선으로 정말 많음. BPM은 이 많은 선들이 (위의 경우 하나의 직선은 절편과 기울기, 두 모수로 결정되니까 모수 공가이 2차원) 균일 분포를 이룬다고 가정하고, 이 분포의 평균을 쓰는 거고, 그게 BPM</li>
<li>그러면 SVM이 딱 마진에 있는 벡터들에만 의존하는 것과 달리 BPM은 마진 뒤에 있는 벡터들에도 영향을 받게된다. Sparse하진 않지만 위의 그림처럼 좀 더 데이터의 퍼짐을 잘 반영하는 classifier를 그릴 수 있는 것.</li>
</ul>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-39.jpg" alt="1"></p>
<ul>
<li>데이터에 담긴 모든 정보를 유한 차원의 모수로 압축하는 방법이 parametric 방법임. 즉 과거의 데이터와 현재의 데이터를 연결하는 유일한 information channel이 바로 $\boldsymbol{\theta}$. 때문에 모델의 복잡도가 제한되어있는데, 데이터의 양이 무제한으로 커질 수도 있는데 이렇게 복잡도를 제한하는 것은 그렇게 유연한 방법은 아님. 데이터에서 오직 제한된 정보만 얻을 수 있기 때문. 비모수적 방법은 모수적 방법의 정반대.</li>
<li>모수적 방법은 과거 데이터의 모든 정보가 모수의 값에 다 반영되기 때문에 model-based임. 즉 뭐냐면 모수의 추정치 혹은 분포만 알고나면 그 학습을 위해 썼던 데이터가 필요가 없다는 것임. 그러나 비모수적 방법은 데이터에 담긴 정보를 모두 그대로 사용하기 때문에 모델을 순차적으로 업데이트하기 위해서는 이전에 쓰인 모든 데이터를 다 알아야함. 즉 memory-based</li>
<li>근래 머신러닝에서 성과를 많이 거둔 알고리즘들은 svm 등 kernel based methods나 뉴럴 네트워크 이런 애들은 다 nonparametric한 방법들이었음 (그도 그럴만한게 데이터가 엄청 많으면 당연히 비모수 방법이 유리하지 모델이 제한되어 있지 않으니까. 못 알아먹어서 문제지만.)</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-40.jpg" alt="1"></p>
<ul>
<li>skip</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-41.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-42.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-43.jpg" alt="1"></p>
<ul>
<li>Gaussian Mixture인데 클래스 개수가 무한히 확장 가능하면 (즉 미리 정해지지 않았다면) 그걸 Dirichlet Process Mixtures라고 하나보다. 다음 시간에 할거다.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-44.jpg" alt="1"></p>
<ul>
<li>이거 다 다음시간에 할건데, 한가지 중요한 것은 모델과 알고리즘은 다르다는 것.
<ul>
<li>모델은 데이터 형성 과정에 대한 설명. 위의 경우 저 모델은 Dirichlet Process Mixtures이고, 저 모델을 학습하는 방법이 바로 EM 알고리즘 등등이다.</li>
</ul>
</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-45.jpg" alt="1"></p>
<ul>
<li>다음에 이어지는 것은 그냥 읽어보자. 아조시가 베이지안 머신러닝 전공하면서 듣다가 빡친 소리들 모아놔서 하나씩 까는 것 같다.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-46.jpg" alt="1"></p>
<ul>
<li>MAP는 사실 그냥 Regularization같은 느낌이 강함. 베이즈 방법의 핵심은 averaging임. 그리고 MLE는 reparameterization에 invariant하지만, MAP는 reparameterization에 variant하다. 그래서 MAP 쓸 바에야 그냥 MLE 써라. Regularization같은 효과를 보고 싶으면 그냥 굳이 prior 갖고 오지 말고 regularization을 바로 써라.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-47.jpg" alt="1"></p>
<ul>
<li>베이지안 추정량에 frequentist 기준을 들어 평가할 수도 있음.</li>
<li>뭐 optimization보다는 averaging이 어렵긴 한데&hellip; 꼭 불가능한 것도 아님.</li>
</ul>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-48.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-49.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-50.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-51.jpg" alt="1"></p>
<p>지금부터 나오는 내용은 강의에도 다루지 않은 부분이다. 이런 주제도 있구나 하고 고개를 끄덕이거나 갸우뚱하고 넘어가자.</p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-52.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-53.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-54.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-55.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-56.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-57.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-58.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-59.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-60.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-61.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-62.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-63.jpg" alt="1"></p>
<p><img src="/posts/mlss2012_ghahramani_bayesian_modelling/mlss2012_ghahramani_bayesian_modelling-64.jpg" alt="1"></p>
<h3 id="references">References</h3>
<ul>
<li><a href="http://videolectures.net/mlss2012_ghahramani_bayesian_modelling/">http://videolectures.net/mlss2012_ghahramani_bayesian_modelling/</a></li>
<li>Pattern Recognition and Machine Learning, Bishop, 2006</li>
<li><a href="http://krasserm.github.io/2019/02/23/bayesian-linear-regression/">http://krasserm.github.io/2019/02/23/bayesian-linear-regression/</a></li>
</ul>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/posterior-approximation/" rel="tag">Posterior Approximation</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="Kang Gyeonghun avatar" src="/mypic1.jpg" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About Kang Gyeonghun</span>
	</div>
	<div class="authorbox__description">
		I study statistics, machine learning, data science or whatever that concerns making inference on infinitie dimension from a limited sample in fintie dimension. This blog is an archive of my journey of study.
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bayesian-ml/week2/03-conjugate-prior-for-multivariate-model/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Conjugate Prior for Multivariate Model</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bayesian-ml/week3/01-discrete-time-markov-chaine-with-finite-state-space/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">(MCMC) Discrete-Time Markov Chain with Finite State Space</p>
		</a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hun-learning94" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/posts/2020-09-07-intro-to-rcpp-and-rcpparmadillo/">Introduction to Rcpp and RcppArmadillo</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-09-06-how-to-add-rtools-to-windows-path-env/">Rtools를 윈도우 환경변수 PATH에 추가하는 방법</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-25-variational-inference/">Variational Inference and Bayesian Gaussian Mixture Model</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/2020-08-24-forward-and-reverse-kl-divergence/">Forward and Reverse KL divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/bayesian-ml/week5/05-mle-minimizes-kl-divergence/">Interpretation of MLE in terms of KL divergence</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/bayesian-ml/week5/04-note-on-kullback-leibler-divergence/">Note on Kullback-Leibler Divergence</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/adaboost/" title="ADABOOST">ADABOOST</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/adaptive-basis-model/" title="Adaptive Basis Model">Adaptive Basis Model</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayes-rule/" title="Bayes Rule">Bayes Rule</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-gmm/" title="Bayesian GMM">Bayesian GMM</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-hierarchy/" title="Bayesian Hierarchy">Bayesian Hierarchy</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bayesian-networks/" title="Bayesian Networks">Bayesian Networks</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/bootstrap/" title="Bootstrap">Bootstrap</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/cart/" title="CART">CART</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/clustering/" title="Clustering">Clustering</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/conjugacy/" title="Conjugacy">Conjugacy</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/d-seperation/" title="D-seperation">D-seperation</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/determinant/" title="Determinant">Determinant</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/diagonalization/" title="Diagonalization">Diagonalization</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/em-algorithm/" title="EM algorithm">EM algorithm</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ensemble-learning/" title="Ensemble Learning">Ensemble Learning</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/frequentist/" title="Frequentist">Frequentist</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gaussian-mixtures/" title="Gaussian Mixtures">Gaussian Mixtures</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/generalized-additive-models/" title="Generalized Additive Models">Generalized Additive Models</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gibbs-sampling/" title="Gibbs Sampling">Gibbs Sampling</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/intro-to-statistical-learning/" title="Intro to Statistical Learning">Intro to Statistical Learning</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/jacobian/" title="Jacobian">Jacobian</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/k-cv/" title="k-CV">k-CV</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/k-means/" title="K-means">K-means</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/kernel/" title="Kernel">Kernel</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/kl-divergence/" title="KL divergence">KL divergence</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lagrangian-duality/" title="Lagrangian Duality">Lagrangian Duality</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lasso/" title="Lasso">Lasso</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/latent-variable/" title="Latent Variable">Latent Variable</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/lda/" title="LDA">LDA</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/linear-adjoint/" title="Linear Adjoint">Linear Adjoint</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/logistic-regression/" title="Logistic Regression">Logistic Regression</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/markov-chain/" title="Markov Chain">Markov Chain</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/matrix-derivatives/" title="Matrix Derivatives">Matrix Derivatives</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mcmc/" title="MCMC">MCMC</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/metropolis-hastings/" title="Metropolis Hastings">Metropolis Hastings</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mse/" title="MSE">MSE</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/multivariate-normal/" title="Multivariate Normal">Multivariate Normal</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/naive-bayes-classifier/" title="Naive Bayes Classifier">Naive Bayes Classifier</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ols/" title="OLS">OLS</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/pca/" title="PCA">PCA</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/posterior-approximation/" title="Posterior Approximation">Posterior Approximation</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/rcpp/" title="Rcpp">Rcpp</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/regression-splines/" title="Regression Splines">Regression Splines</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ridge/" title="Ridge">Ridge</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/self-adjoint/" title="Self Adjoint">Self Adjoint</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/similar-matrices/" title="Similar Matrices">Similar Matrices</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/singular-value-decomposition/" title="Singular Value Decomposition">Singular Value Decomposition</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/stochastic-process/" title="Stochastic Process">Stochastic Process</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/svm/" title="SVM">SVM</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/vector-derivatives/" title="vector derivatives">vector derivatives</a>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2020 Kang Gyeonghun.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>